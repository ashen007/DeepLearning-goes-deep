{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU, ELU\n",
    "\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Nadam, SGD, Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import array_to_img, img_to_array, load_img"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2022-04-22T07:17:08.382676Z",
     "iopub.execute_input": "2022-04-22T07:17:08.383534Z",
     "iopub.status.idle": "2022-04-22T07:17:08.393627Z",
     "shell.execute_reply.started": "2022-04-22T07:17:08.383497Z",
     "shell.execute_reply": "2022-04-22T07:17:08.392635Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def load_img_to_array(path):\n",
    "    return img_to_array(load_img(path))\n",
    "\n",
    "\n",
    "def resize(image, size):\n",
    "    return tf.image.resize(image, size)\n",
    "\n",
    "\n",
    "def aggressive_cropping(image, copies, crop_window, resize_smallest_side, output_shape):\n",
    "    global img, resized_copies, crops\n",
    "\n",
    "    if isinstance(resize_smallest_side, int):\n",
    "        img = resize(image, (resize_smallest_side, resize_smallest_side))\n",
    "\n",
    "    if isinstance(resize_smallest_side, (list, tuple)):\n",
    "        resized_copies = [tf.image.resize(image, (size, size)) for size in resize_smallest_side]\n",
    "\n",
    "    if isinstance(crop_window, int):\n",
    "        if isinstance(resize_smallest_side, int):\n",
    "            crops = [tf.image.random_crop(img, crop_window) for _ in range(copies)]\n",
    "        elif isinstance(resize_smallest_side, (list, tuple)):\n",
    "            crops = [tf.image.random_crop(img_, crop_window) for _ in range(copies) for img_ in\n",
    "                     resized_copies]\n",
    "\n",
    "    elif isinstance(crop_window, (list, tuple)):\n",
    "        if isinstance(resize_smallest_side, int):\n",
    "            crops = [tf.image.random_crop(img, crop_window) for _ in range(copies)]\n",
    "        elif isinstance(resize_smallest_side, (list, tuple)):\n",
    "            crops = [tf.image.random_crop(img_, crop_window) for _ in range(copies) for img_ in resized_copies]\n",
    "\n",
    "    return [resize(crop_img, output_shape) for crop_img in crops]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-22T05:51:02.890277Z",
     "iopub.execute_input": "2022-04-22T05:51:02.890659Z",
     "iopub.status.idle": "2022-04-22T05:51:02.908042Z",
     "shell.execute_reply.started": "2022-04-22T05:51:02.890614Z",
     "shell.execute_reply": "2022-04-22T05:51:02.903595Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "os.mkdir('dataset_1')\n",
    "os.mkdir('dataset_2')\n",
    "\n",
    "os.mkdir('dataset_1/train')\n",
    "os.mkdir('dataset_1/valid')\n",
    "\n",
    "os.mkdir('dataset_2/train')\n",
    "os.mkdir('dataset_2/valid')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-22T05:51:02.910325Z",
     "iopub.execute_input": "2022-04-22T05:51:02.911010Z",
     "iopub.status.idle": "2022-04-22T05:51:02.927741Z",
     "shell.execute_reply.started": "2022-04-22T05:51:02.910966Z",
     "shell.execute_reply": "2022-04-22T05:51:02.926683Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for k, sub_class in enumerate(os.listdir('../input/100-bird-species/valid/')):\n",
    "    sub_path = os.path.join('../input/100-bird-species/valid/', sub_class)\n",
    "    dst = os.path.join('dataset_1/valid/', sub_class)\n",
    "\n",
    "    if not os.path.isdir(dst):\n",
    "        os.mkdir(dst)\n",
    "\n",
    "    for file in os.listdir(sub_path):\n",
    "        file_path = os.path.join(sub_path, file)\n",
    "        img_arr = load_img_to_array(file_path)\n",
    "        copies = aggressive_cropping(img_arr, 3, [128, 128, 3], [224, 224], [32, 32])\n",
    "\n",
    "        for i, copy in enumerate(copies):\n",
    "            array_to_img(copy).save(os.path.join(dst, f'c_{i}_{file}'))\n",
    "\n",
    "        array_to_img(resize(img_arr, (32, 32))).save(os.path.join(dst, file))\n",
    "\n",
    "    print(k + 1, end='\\r')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-22T06:21:21.846131Z",
     "iopub.execute_input": "2022-04-22T06:21:21.846447Z",
     "iopub.status.idle": "2022-04-22T06:22:12.593958Z",
     "shell.execute_reply.started": "2022-04-22T06:21:21.846407Z",
     "shell.execute_reply": "2022-04-22T06:22:12.592997Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for k, sub_class in enumerate(os.listdir('../input/intel-image-classification/seg_test/seg_test/')):\n",
    "    sub_path = os.path.join('../input/intel-image-classification/seg_test/seg_test/', sub_class)\n",
    "    dst = os.path.join('dataset_2/valid/', sub_class)\n",
    "\n",
    "    if not os.path.isdir(dst):\n",
    "        os.mkdir(dst)\n",
    "\n",
    "    for file in os.listdir(sub_path):\n",
    "        file_path = os.path.join(sub_path, file)\n",
    "        img_arr = load_img_to_array(file_path)\n",
    "        copies = aggressive_cropping(img_arr, 3, [128, 128, 3], [224, 224], [32, 32])\n",
    "\n",
    "        for i, copy in enumerate(copies):\n",
    "            array_to_img(copy).save(os.path.join(dst, f'c_{i}_{file}'))\n",
    "\n",
    "        array_to_img(resize(img_arr, (32, 32))).save(os.path.join(dst, file))\n",
    "\n",
    "    print(k + 1, end='\\r')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-04-22T06:22:12.596003Z",
     "iopub.execute_input": "2022-04-22T06:22:12.596313Z",
     "iopub.status.idle": "2022-04-22T06:23:20.440214Z",
     "shell.execute_reply.started": "2022-04-22T06:22:12.596269Z",
     "shell.execute_reply": "2022-04-22T06:23:20.438286Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_generator = ImageDataGenerator(rescale=1 / 255.)\n",
    "\n",
    "valid_generator = ImageDataGenerator(rescale=1 / 255.)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-22T06:23:54.629078Z",
     "iopub.execute_input": "2022-04-22T06:23:54.629403Z",
     "iopub.status.idle": "2022-04-22T06:23:54.635997Z",
     "shell.execute_reply.started": "2022-04-22T06:23:54.629360Z",
     "shell.execute_reply": "2022-04-22T06:23:54.634713Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_set = train_generator.flow_from_directory(directory='dataset_1/train',\n",
    "                                                target_size=(32, 32),\n",
    "                                                batch_size=32,\n",
    "                                                subset='training')\n",
    "\n",
    "val_set = valid_generator.flow_from_directory(directory='dataset_1/valid',\n",
    "                                              target_size=(32, 32),\n",
    "                                              batch_size=32)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-22T07:28:09.628783Z",
     "iopub.execute_input": "2022-04-22T07:28:09.629092Z",
     "iopub.status.idle": "2022-04-22T07:28:39.516138Z",
     "shell.execute_reply.started": "2022-04-22T07:28:09.629060Z",
     "shell.execute_reply": "2022-04-22T07:28:39.514997Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_set_1 = train_generator.flow_from_directory(directory='dataset_2/train',\n",
    "                                                  target_size=(32, 32),\n",
    "                                                  batch_size=128,\n",
    "                                                 color_mode='grayscale')\n",
    "\n",
    "val_set_1 = valid_generator.flow_from_directory(directory='dataset_2/valid',\n",
    "                                                target_size=(32, 32),\n",
    "                                                batch_size=128,\n",
    "                                               color_mode='grayscale')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-22T03:31:10.850491Z",
     "iopub.execute_input": "2022-04-22T03:31:10.851013Z",
     "iopub.status.idle": "2022-04-22T03:31:16.789625Z",
     "shell.execute_reply.started": "2022-04-22T03:31:10.850974Z",
     "shell.execute_reply": "2022-04-22T03:31:16.788867Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "fixed_data = []\n",
    "\n",
    "for i, file in enumerate(os.listdir('../input/intel-image-classification/seg_pred/seg_pred/')):\n",
    "    if i == 64:\n",
    "        break\n",
    "    else:\n",
    "        img_arr = img_to_array(load_img(os.path.join('../input/intel-image-classification/seg_pred/seg_pred',file),\n",
    "                                        color_mode='grayscale',\n",
    "                                       target_size=(32,32)))\n",
    "        fixed_data.append(img_arr)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-22T07:28:46.953680Z",
     "iopub.execute_input": "2022-04-22T07:28:46.954330Z",
     "iopub.status.idle": "2022-04-22T07:28:47.536757Z",
     "shell.execute_reply.started": "2022-04-22T07:28:46.954296Z",
     "shell.execute_reply": "2022-04-22T07:28:47.535729Z"
    },
    "trusted": true
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ***Models***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "init = HeNormal()\n",
    "classes = [6, 400]\n",
    "act_name = ['relu', 'lrelu', 'elu']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-04-22T10:13:00.219425Z",
     "iopub.execute_input": "2022-04-22T10:13:00.219901Z",
     "iopub.status.idle": "2022-04-22T10:13:00.226531Z",
     "shell.execute_reply.started": "2022-04-22T10:13:00.219838Z",
     "shell.execute_reply": "2022-04-22T10:13:00.225398Z"
    },
    "trusted": true
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                             factor=np.exp(-0.1),\n",
    "                              patience=5)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-22T07:19:38.301699Z",
     "iopub.execute_input": "2022-04-22T07:19:38.302515Z",
     "iopub.status.idle": "2022-04-22T07:19:38.307705Z",
     "shell.execute_reply.started": "2022-04-22T07:19:38.302464Z",
     "shell.execute_reply": "2022-04-22T07:19:38.306204Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class AvgUnitActivations(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, subset):\n",
    "        self.train_subset = subset\n",
    "        self.activations = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        out_layer = self.model.layers[-2]\n",
    "        activations = Model(self.model.input, out_layer.output)(self.train_subset)\n",
    "        self.activations.append(activations)\n",
    "\n",
    "avg_activation = AvgUnitActivations(tf.Variable(fixed_data, shape=(64,32,32,1)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-04-22T07:28:49.745343Z",
     "iopub.execute_input": "2022-04-22T07:28:49.745984Z",
     "iopub.status.idle": "2022-04-22T07:28:49.865072Z",
     "shell.execute_reply.started": "2022-04-22T07:28:49.745943Z",
     "shell.execute_reply": "2022-04-22T07:28:49.864015Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "activation_learn_history_0 = {}\n",
    "\n",
    "for act in act_name:\n",
    "    learning_history = []\n",
    "\n",
    "    for l in range(3):\n",
    "        model1 = Sequential()\n",
    "\n",
    "        model1.add(InputLayer((32, 32, 1)))\n",
    "        model1.add(Flatten())\n",
    "\n",
    "        for _ in range(8):\n",
    "            model1.add(Dense(units=128, kernel_initializer=init))\n",
    "\n",
    "            if act == 'relu':\n",
    "                model1.add(ReLU())\n",
    "            elif act == 'lrelu':\n",
    "                model1.add(LeakyReLU(alpha=0.1))\n",
    "            elif act == 'elu':\n",
    "                model1.add(ELU())\n",
    "\n",
    "        model1.add(Dense(units=classes[0], activation='softmax'))\n",
    "\n",
    "        model1.compile(optimizer=SGD(learning_rate=0.01),\n",
    "                       loss=categorical_crossentropy,\n",
    "                       metrics=['accuracy'])\n",
    "        \n",
    "        print(f'{l+1}/3')\n",
    "\n",
    "        history = model1.fit(train_set_1,\n",
    "                             validation_data=val_set_1,\n",
    "                             steps_per_epoch=256,\n",
    "                             validation_steps=128,\n",
    "                             epochs=50,\n",
    "                             callbacks=[avg_activation])\n",
    "\n",
    "        learning_history.append(history.history)\n",
    "\n",
    "    activation_learn_history_0[act] = learning_history\n",
    "\n",
    "with open('result_0.json', 'w') as file:\n",
    "    json.dump(activation_learn_history_0, file)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "active = avg_activation.activations\n",
    "\n",
    "with open('unit activations.pkl', 'wb') as f:\n",
    "    pickle.dump(active, f)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T18:13:09.716369Z",
     "iopub.execute_input": "2022-04-21T18:13:09.716669Z",
     "iopub.status.idle": "2022-04-21T18:13:09.802538Z",
     "shell.execute_reply.started": "2022-04-21T18:13:09.716636Z",
     "shell.execute_reply": "2022-04-21T18:13:09.80151Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "activation_learn_history_1 = {}\n",
    "\n",
    "for act in act_name:\n",
    "    learning_history = []\n",
    "\n",
    "    for l in range(3):\n",
    "        model2 = Sequential()\n",
    "\n",
    "        model2.add(InputLayer((32, 32, 3)))\n",
    "        model2.add(Conv2D(filters=192, kernel_size=(5, 5), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005), padding='same'))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'elu':\n",
    "            model2.add(ELU())\n",
    "\n",
    "        model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2.add(Dropout(rate=0.0))\n",
    "\n",
    "        model2.add(Conv2D(filters=192, kernel_size=(1, 1), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005), padding='same'))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'elu':\n",
    "            model2.add(ELU())\n",
    "\n",
    "        model2.add(Conv2D(filters=240, kernel_size=(3, 3), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005), padding='same'))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'elu':\n",
    "            model2.add(ELU())\n",
    "\n",
    "        model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2.add(Dropout(rate=0.1))\n",
    "\n",
    "        model2.add(Conv2D(filters=240, kernel_size=(1, 1), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005), padding='same'))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'elu':\n",
    "            model2.add(ELU())\n",
    "\n",
    "        model2.add(Conv2D(filters=260, kernel_size=(2, 2), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005), padding='same'))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'elu':\n",
    "            model2.add(ELU())\n",
    "\n",
    "        model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2.add(Dropout(rate=0.2))\n",
    "\n",
    "        model2.add(Conv2D(filters=260, kernel_size=(1, 1), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005), padding='same'))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'elu':\n",
    "            model2.add(ELU())\n",
    "\n",
    "        model2.add(Conv2D(filters=280, kernel_size=(2, 2), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005), padding='same'))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'elu':\n",
    "            model2.add(ELU())\n",
    "\n",
    "        model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2.add(Dropout(rate=0.3))\n",
    "\n",
    "        model2.add(Conv2D(filters=280, kernel_size=(1, 1), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005), padding='same'))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'elu':\n",
    "            model2.add(ELU())\n",
    "\n",
    "        model2.add(Conv2D(filters=300, kernel_size=(2, 2), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005), padding='same'))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'elu':\n",
    "            model2.add(ELU())\n",
    "\n",
    "        model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2.add(Dropout(rate=0.4))\n",
    "\n",
    "        model2.add(Conv2D(filters=300, kernel_size=(1, 1), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005), padding='same'))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'elu':\n",
    "            model2.add(ELU())\n",
    "\n",
    "        model2.add(Dropout(rate=0.5))\n",
    "\n",
    "        model2.add(Flatten())\n",
    "        model2.add(Dense(classes[1], activation='softmax'))\n",
    "\n",
    "        model2.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9),\n",
    "                       loss=categorical_crossentropy,\n",
    "                       metrics=['accuracy'])\n",
    "        \n",
    "        print(f'{l+1}/3')\n",
    "\n",
    "        history = model2.fit(train_set,\n",
    "                             validation_data=val_set,\n",
    "                             steps_per_epoch=156,\n",
    "                             validation_steps=64,\n",
    "                             epochs=300,\n",
    "                             batch_size=32,\n",
    "                             callbacks=[reduce_lr])\n",
    "\n",
    "        learning_history.append(history.history)\n",
    "\n",
    "    activation_learn_history_1[act] = learning_history\n",
    "\n",
    "with open('result_1.json', 'w') as file:\n",
    "    json.dump(activation_learn_history_1, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-04-22T10:13:07.842934Z",
     "iopub.execute_input": "2022-04-22T10:13:07.843225Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "activation_learn_history_2 = {}\n",
    "\n",
    "for act in act_name:\n",
    "    learning_history = []\n",
    "\n",
    "    for l in range(3):\n",
    "        model2_bn = Sequential()\n",
    "\n",
    "        model2_bn.add(InputLayer((32, 32, 3)))\n",
    "        model2_bn.add(\n",
    "            Conv2D(filters=192, kernel_size=(5, 5), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005)))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2_bn.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2_bn.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'elu':\n",
    "            model2_bn.add(ELU())\n",
    "\n",
    "        model2_bn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2_bn.add(Dropout(rate=0.0))\n",
    "        model2_bn.add(BatchNormalization())\n",
    "\n",
    "        model2_bn.add(\n",
    "            Conv2D(filters=192, kernel_size=(1, 1), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005)))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2_bn.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2_bn.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'elu':\n",
    "            model2_bn.add(ELU())\n",
    "\n",
    "        model2_bn.add(\n",
    "            Conv2D(filters=240, kernel_size=(3, 3), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005)))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2_bn.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2_bn.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'elu':\n",
    "            model2_bn.add(ELU())\n",
    "\n",
    "        model2_bn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2_bn.add(Dropout(rate=0.1))\n",
    "        model2_bn.add(BatchNormalization())\n",
    "\n",
    "        model2_bn.add(\n",
    "            Conv2D(filters=240, kernel_size=(1, 1), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005)))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2_bn.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2_bn.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'elu':\n",
    "            model2_bn.add(ELU())\n",
    "\n",
    "        model2_bn.add(\n",
    "            Conv2D(filters=260, kernel_size=(2, 2), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005)))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2_bn.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2_bn.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'elu':\n",
    "            model2_bn.add(ELU())\n",
    "\n",
    "        model2_bn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2_bn.add(Dropout(rate=0.2))\n",
    "        model2_bn.add(BatchNormalization())\n",
    "\n",
    "        model2_bn.add(\n",
    "            Conv2D(filters=260, kernel_size=(1, 1), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005)))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2_bn.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2_bn.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'elu':\n",
    "            model2_bn.add(ELU())\n",
    "\n",
    "        model2_bn.add(\n",
    "            Conv2D(filters=280, kernel_size=(2, 2), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005)))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2_bn.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2_bn.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'elu':\n",
    "            model2_bn.add(ELU())\n",
    "\n",
    "        model2_bn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2_bn.add(Dropout(rate=0.3))\n",
    "        model2_bn.add(BatchNormalization())\n",
    "\n",
    "        model2_bn.add(\n",
    "            Conv2D(filters=280, kernel_size=(1, 1), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005)))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2_bn.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2_bn.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'elu':\n",
    "            model2_bn.add(ELU())\n",
    "\n",
    "        model2_bn.add(\n",
    "            Conv2D(filters=300, kernel_size=(2, 2), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005)))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2_bn.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2_bn.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'elu':\n",
    "            model2_bn.add(ELU())\n",
    "\n",
    "        model2_bn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2_bn.add(Dropout(rate=0.4))\n",
    "        model2_bn.add(BatchNormalization())\n",
    "\n",
    "        model2_bn.add(\n",
    "            Conv2D(filters=300, kernel_size=(1, 1), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005)))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2_bn.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2_bn.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'elu':\n",
    "            model2_bn.add(ELU())\n",
    "\n",
    "        model2_bn.add(Dropout(rate=0.5))\n",
    "        model2_bn.add(BatchNormalization())\n",
    "\n",
    "        model2_bn.add(Flatten())\n",
    "        model2_bn.add(Dense(filters=classes[1], activation='softmax'))\n",
    "\n",
    "        model2_bn.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9),\n",
    "                          loss=categorical_crossentropy,\n",
    "                          metrics=['accuracy'])\n",
    "        \n",
    "        print(f'{l+1}/3')\n",
    "\n",
    "        history = model2_bn.fit(train_set,\n",
    "                                validation_data=val_set,\n",
    "                                steps_per_epoch=128,\n",
    "                                validation_steps=64,\n",
    "                                epochs=150,\n",
    "                                batch_size=32,\n",
    "                                callbacks=[reduce_lr])\n",
    "\n",
    "        learning_history.append(history.history)\n",
    "\n",
    "    activation_learn_history_2[act] = learning_history\n",
    "\n",
    "with open('result_2.json', 'w') as file:\n",
    "    json.dump(activation_learn_history_2, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "0764362936",
   "metadata": {}
  }
 ]
}