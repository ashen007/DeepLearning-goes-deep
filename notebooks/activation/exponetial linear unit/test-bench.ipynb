{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "from tensorflow.keras.layers import ReLU,LeakyReLU,ELU\n",
    "\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Nadam, SGD, Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.utils import array_to_img, img_to_array, load_img"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2022-04-19T05:16:50.272471Z",
     "iopub.execute_input": "2022-04-19T05:16:50.272727Z",
     "iopub.status.idle": "2022-04-19T05:16:50.28441Z",
     "shell.execute_reply.started": "2022-04-19T05:16:50.27269Z",
     "shell.execute_reply": "2022-04-19T05:16:50.283771Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def load_img_to_array(path):\n",
    "    return img_to_array(load_img(path))\n",
    "\n",
    "\n",
    "def resize(image, size):\n",
    "    return tf.image.resize(image, size)\n",
    "\n",
    "\n",
    "def aggressive_cropping(image, copies, crop_window, resize_smallest_side, output_shape):\n",
    "    global img, resized_copies, crops\n",
    "\n",
    "    if isinstance(resize_smallest_side, int):\n",
    "        img = resize(image, (resize_smallest_side, resize_smallest_side))\n",
    "\n",
    "    if isinstance(resize_smallest_side, (list, tuple)):\n",
    "        resized_copies = [tf.image.resize(image, (size, size)) for size in resize_smallest_side]\n",
    "\n",
    "    if isinstance(crop_window, int):\n",
    "        if isinstance(resize_smallest_side, int):\n",
    "            crops = [tf.image.random_crop(img, crop_window) for _ in range(copies)]\n",
    "        elif isinstance(resize_smallest_side, (list, tuple)):\n",
    "            crops = [tf.image.random_crop(img_, crop_window) for _ in range(copies) for img_ in\n",
    "                     resized_copies]\n",
    "\n",
    "    elif isinstance(crop_window, (list, tuple)):\n",
    "        if isinstance(resize_smallest_side, int):\n",
    "            crops = [tf.image.random_crop(img, crop_window) for _ in range(copies)]\n",
    "        elif isinstance(resize_smallest_side, (list, tuple)):\n",
    "            crops = [tf.image.random_crop(img_, crop_window) for _ in range(copies) for img_ in resized_copies]\n",
    "\n",
    "    return [resize(crop_img, output_shape) for crop_img in crops]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-19T05:16:52.059105Z",
     "iopub.execute_input": "2022-04-19T05:16:52.059728Z",
     "iopub.status.idle": "2022-04-19T05:16:52.06968Z",
     "shell.execute_reply.started": "2022-04-19T05:16:52.059688Z",
     "shell.execute_reply": "2022-04-19T05:16:52.068699Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "os.mkdir('dataset_1')\n",
    "os.mkdir('dataset_2')\n",
    "\n",
    "os.mkdir('dataset_1/train')\n",
    "os.mkdir('dataset_1/valid')\n",
    "\n",
    "os.mkdir('dataset_2/train')\n",
    "os.mkdir('dataset_2/valid')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-19T05:16:56.060468Z",
     "iopub.execute_input": "2022-04-19T05:16:56.061008Z",
     "iopub.status.idle": "2022-04-19T05:16:56.066041Z",
     "shell.execute_reply.started": "2022-04-19T05:16:56.060967Z",
     "shell.execute_reply": "2022-04-19T05:16:56.065269Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for k, sub_class in enumerate(os.listdir('../input/100-bird-species/train/')):\n",
    "    sub_path = os.path.join('../input/100-bird-species/train', sub_class)\n",
    "    dst = os.path.join('dataset_1/train', sub_class)\n",
    "\n",
    "    if not os.path.isdir(dst):\n",
    "        os.mkdir(dst)\n",
    "\n",
    "    for file in os.listdir(sub_path):\n",
    "        file_path = os.path.join(sub_path, file)\n",
    "        img_arr = load_img_to_array(file_path)\n",
    "        copies = aggressive_cropping(img_arr, 3, [128, 128, 3], [224, 224], [32, 32])\n",
    "\n",
    "        for i, copy in enumerate(copies):\n",
    "            array_to_img(copy).save(os.path.join(dst, f'c_{i}_{file}'))\n",
    "\n",
    "        array_to_img(resize(img_arr, (32, 32))).save(os.path.join(dst, file))\n",
    "\n",
    "    print(k + 1, end='\\r')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-19T05:17:05.479978Z",
     "iopub.execute_input": "2022-04-19T05:17:05.480231Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for k, sub_class in enumerate(os.listdir('')):\n",
    "    sub_path = os.path.join('', sub_class)\n",
    "    dst = os.path.join('dataset_2/train', sub_class)\n",
    "\n",
    "    if not os.path.isdir(dst):\n",
    "        os.mkdir(dst)\n",
    "\n",
    "    for file in os.listdir(sub_path):\n",
    "        file_path = os.path.join(sub_path, file)\n",
    "        img_arr = load_img_to_array(file_path)\n",
    "        copies = aggressive_cropping(img_arr, 3, [128, 128, 3], [224, 224], [32, 32])\n",
    "\n",
    "        for i, copy in enumerate(copies):\n",
    "            array_to_img(copy).save(os.path.join(dst, f'c_{i}_{file}'))\n",
    "\n",
    "        array_to_img(resize(img_arr, (32, 32))).save(os.path.join(dst, file))\n",
    "\n",
    "    print(k + 1, end='\\r')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_generator = ImageDataGenerator(rescale=1 / 255.,\n",
    "                                     horizontal_flip=True)\n",
    "\n",
    "valid_generator = ImageDataGenerator(rescale=1 / 255.,\n",
    "                                     horizontal_flip=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-18T12:12:49.10647Z",
     "iopub.execute_input": "2022-04-18T12:12:49.107291Z",
     "iopub.status.idle": "2022-04-18T12:12:49.112743Z",
     "shell.execute_reply.started": "2022-04-18T12:12:49.107249Z",
     "shell.execute_reply": "2022-04-18T12:12:49.111197Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_set = train_generator.flow_from_directory('../input/intel-image-classification/seg_train/seg_train/',\n",
    "                                                target_size=(224, 224),\n",
    "                                                batch_size=32)\n",
    "\n",
    "val_set = valid_generator.flow_from_directory('../input/intel-image-classification/seg_test/seg_test/',\n",
    "                                              target_size=(224, 224),\n",
    "                                              batch_size=32)\n",
    "\n",
    "train_set_1 = train_generator.flow_from_directory('../input/intel-image-classification/seg_train/seg_train/',\n",
    "                                                target_size=(224, 224),\n",
    "                                                batch_size=32)\n",
    "\n",
    "val_set_1 = valid_generator.flow_from_directory('../input/intel-image-classification/seg_test/seg_test/',\n",
    "                                              target_size=(224, 224),\n",
    "                                              batch_size=32)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-18T12:12:51.330311Z",
     "iopub.execute_input": "2022-04-18T12:12:51.330565Z",
     "iopub.status.idle": "2022-04-18T12:13:10.421718Z",
     "shell.execute_reply.started": "2022-04-18T12:12:51.330537Z",
     "shell.execute_reply": "2022-04-18T12:13:10.420994Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ***Models***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "init = HeNormal()\n",
    "classes = [6, 400]\n",
    "act_name = ['relu','lrelu','elu']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def scheduler(epoch,lr):\n",
    "    if epoch<35000:\n",
    "        return 0.01\n",
    "    elif 35000<epoch<85000:\n",
    "        return 0.005\n",
    "    elif 85000<epoch<135000:\n",
    "        return 0.0005\n",
    "    elif 135000<epoch<165000:\n",
    "        return 0.00005\n",
    "\n",
    "lr_schedule = LearningRateScheduler(scheduler)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "activation_learn_history_0 = {}\n",
    "\n",
    "for act in act_name:\n",
    "    learning_history = []\n",
    "\n",
    "    for _ in range(10):\n",
    "        model1 = Sequential()\n",
    "\n",
    "        model1.add(InputLayer((32, 32, 1)))\n",
    "        model1.add(Flatten())\n",
    "\n",
    "        for _ in range(8):\n",
    "            model1.add(Dense(units=128, kernel_initializer=init))\n",
    "\n",
    "            if act=='relu':\n",
    "                model1.add(ReLU())\n",
    "            elif act=='lrelu':\n",
    "                model1.add(LeakyReLU(alpha=0.1))\n",
    "            elif act=='elu':\n",
    "                model1.add(ELU())\n",
    "\n",
    "        model1.add(Dense(units=classes[0], activation='softmax'))\n",
    "\n",
    "        model1.compile(optimizer=SGD(learning_rate=0.01),\n",
    "                       loss=categorical_crossentropy,\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "        history = model1.fit(train_set_1,\n",
    "                             validation_data=val_set_1,\n",
    "                             steps_per_epoch=256,\n",
    "                             validation_steps=128,\n",
    "                             epochs=300,\n",
    "                             batch_size=64)\n",
    "\n",
    "        learning_history.append(history.history)\n",
    "\n",
    "    activation_learn_history_0[act_name[i]] = learning_history\n",
    "\n",
    "with open('result_0.json','w') as file:\n",
    "    json.dump(activation_learn_history_0,file)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-18T12:18:50.887107Z",
     "iopub.execute_input": "2022-04-18T12:18:50.887716Z",
     "iopub.status.idle": "2022-04-18T12:18:50.913581Z",
     "shell.execute_reply.started": "2022-04-18T12:18:50.887654Z",
     "shell.execute_reply": "2022-04-18T12:18:50.912825Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "activation_learn_history_1 = {}\n",
    "\n",
    "for i in range(3):\n",
    "    learning_history = []\n",
    "\n",
    "    for _ in range(10):\n",
    "        model2 = Sequential()\n",
    "\n",
    "        model2.add(InputLayer((32, 32, 3)))\n",
    "        model2.add(Conv2D(filters=192, kernel_size=(5, 5), kernel_initializer=init, activation=activations[i],\n",
    "                          kernel_regularizer=l2(l2=0.0005)))\n",
    "        model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2.add(Dropout(rate=0.0))\n",
    "\n",
    "        model2.add(Conv2D(filters=192, kernel_size=(1, 1), kernel_initializer=init, activation=activations[i],\n",
    "                          kernel_regularizer=l2(l2=0.0005)))\n",
    "        model2.add(Conv2D(filters=240, kernel_size=(3, 3), kernel_initializer=init, activation=activations[i],\n",
    "                          kernel_regularizer=l2(l2=0.0005)))\n",
    "        model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2.add(Dropout(rate=0.1))\n",
    "\n",
    "        model2.add(Conv2D(filters=240, kernel_size=(1, 1), kernel_initializer=init, activation=activations[i],\n",
    "                          kernel_regularizer=l2(l2=0.0005)))\n",
    "        model2.add(Conv2D(filters=260, kernel_size=(2, 2), kernel_initializer=init, activation=activations[i],\n",
    "                          kernel_regularizer=l2(l2=0.0005)))\n",
    "        model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2.add(Dropout(rate=0.2))\n",
    "\n",
    "        model2.add(Conv2D(filters=260, kernel_size=(1, 1), kernel_initializer=init, activation=activations[i],\n",
    "                          kernel_regularizer=l2(l2=0.0005)))\n",
    "        model2.add(Conv2D(filters=280, kernel_size=(2, 2), kernel_initializer=init, activation=activations[i],\n",
    "                          kernel_regularizer=l2(l2=0.0005)))\n",
    "        model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2.add(Dropout(rate=0.3))\n",
    "\n",
    "        model2.add(Conv2D(filters=280, kernel_size=(1, 1), kernel_initializer=init, activation=activations[i],\n",
    "                          kernel_regularizer=l2(l2=0.0005)))\n",
    "        model2.add(Conv2D(filters=300, kernel_size=(2, 2), kernel_initializer=init, activation=activations[i],\n",
    "                          kernel_regularizer=l2(l2=0.0005)))\n",
    "        model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2.add(Dropout(rate=0.4))\n",
    "\n",
    "        model2.add(Conv2D(filters=300, kernel_size=(1, 1), kernel_initializer=init, activation=activations[i],\n",
    "                          kernel_regularizer=l2(l2=0.0005)))\n",
    "        model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2.add(Dropout(rate=0.5))\n",
    "\n",
    "        model2.add(Flatten())\n",
    "        model2.add(Dense(filters=classes[1], activation=softmax))\n",
    "\n",
    "        model2.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9),\n",
    "                       loss=categorical_crossentropy,\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "        history = model2.fit(train_set,\n",
    "                             validation_data=val_set,\n",
    "                             steps_per_epoch=256,\n",
    "                             validation_steps=128,\n",
    "                             epochs=165000,\n",
    "                             batch_size=128,\n",
    "                             callbacks=[lr_schedule])\n",
    "\n",
    "        learning_history.append(history.history)\n",
    "\n",
    "    activation_learn_history_1[act_name[i]] = learning_history\n",
    "\n",
    "with open('result_1.json','w') as file:\n",
    "    json.dump(activation_learn_history_1,file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "activation_learn_history_2 = {}\n",
    "\n",
    "for i in range(3):\n",
    "    learning_history = []\n",
    "\n",
    "    for _ in range(10):\n",
    "        model2_bn = Sequential()\n",
    "\n",
    "        model2_bn.add(InputLayer((32, 32, 3)))\n",
    "        model2_bn.add(Conv2D(filters=192, kernel_size=(5, 5), kernel_initializer=init, activation=activations[i],\n",
    "                          kernel_regularizer=l2(l2=0.0005)))\n",
    "        model2_bn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2_bn.add(Dropout(rate=0.0))\n",
    "        model2_bn.add(BatchNormalization())\n",
    "\n",
    "        model2_bn.add(Conv2D(filters=192, kernel_size=(1, 1), kernel_initializer=init, activation=activations[i],\n",
    "                          kernel_regularizer=l2(l2=0.0005)))\n",
    "        model2_bn.add(Conv2D(filters=240, kernel_size=(3, 3), kernel_initializer=init, activation=activations[i],\n",
    "                          kernel_regularizer=l2(l2=0.0005)))\n",
    "        model2_bn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2_bn.add(Dropout(rate=0.1))\n",
    "        model2_bn.add(BatchNormalization())\n",
    "\n",
    "        model2_bn.add(Conv2D(filters=240, kernel_size=(1, 1), kernel_initializer=init, activation=activations[i],\n",
    "                          kernel_regularizer=l2(l2=0.0005)))\n",
    "        model2_bn.add(Conv2D(filters=260, kernel_size=(2, 2), kernel_initializer=init, activation=activations[i],\n",
    "                          kernel_regularizer=l2(l2=0.0005)))\n",
    "        model2_bn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2_bn.add(Dropout(rate=0.2))\n",
    "        model2_bn.add(BatchNormalization())\n",
    "\n",
    "        model2_bn.add(Conv2D(filters=260, kernel_size=(1, 1), kernel_initializer=init, activation=activations[i],\n",
    "                          kernel_regularizer=l2(l2=0.0005)))\n",
    "        model2_bn.add(Conv2D(filters=280, kernel_size=(2, 2), kernel_initializer=init, activation=activations[i],\n",
    "                          kernel_regularizer=l2(l2=0.0005)))\n",
    "        model2_bn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2_bn.add(Dropout(rate=0.3))\n",
    "        model2_bn.add(BatchNormalization())\n",
    "\n",
    "        model2_bn.add(Conv2D(filters=280, kernel_size=(1, 1), kernel_initializer=init, activation=activations[i],\n",
    "                          kernel_regularizer=l2(l2=0.0005)))\n",
    "        model2_bn.add(Conv2D(filters=300, kernel_size=(2, 2), kernel_initializer=init, activation=activations[i],\n",
    "                          kernel_regularizer=l2(l2=0.0005)))\n",
    "        model2_bn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2_bn.add(Dropout(rate=0.4))\n",
    "        model2_bn.add(BatchNormalization())\n",
    "\n",
    "        model2_bn.add(Conv2D(filters=300, kernel_size=(1, 1), kernel_initializer=init, activation=activations[i],\n",
    "                          kernel_regularizer=l2(l2=0.0005)))\n",
    "        model2_bn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2_bn.add(Dropout(rate=0.5))\n",
    "        model2_bn.add(BatchNormalization())\n",
    "\n",
    "        model2_bn.add(Flatten())\n",
    "        model2_bn.add(Dense(filters=classes[1], activation=softmax))\n",
    "\n",
    "        model2_bn.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9),\n",
    "                       loss=categorical_crossentropy,\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "        history = model2_bn.fit(train_set,\n",
    "                                validation_data=val_set,\n",
    "                                steps_per_epoch=256,\n",
    "                                validation_steps=128,\n",
    "                                epochs=165000,\n",
    "                                batch_size=128,\n",
    "                                callbacks=[lr_schedule])\n",
    "\n",
    "        learning_history.append(history.history)\n",
    "\n",
    "    activation_learn_history_2[act_name[i]] = learning_history\n",
    "\n",
    "with open('result_2.json','w') as file:\n",
    "    json.dump(activation_learn_history_2,file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}