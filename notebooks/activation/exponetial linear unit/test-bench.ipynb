{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU, ELU\n",
    "\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Nadam, SGD, Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.utils import array_to_img, img_to_array, load_img"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2022-04-21T09:18:28.382523Z",
     "iopub.execute_input": "2022-04-21T09:18:28.383005Z",
     "iopub.status.idle": "2022-04-21T09:18:35.692116Z",
     "shell.execute_reply.started": "2022-04-21T09:18:28.382902Z",
     "shell.execute_reply": "2022-04-21T09:18:35.691105Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def load_img_to_array(path):\n",
    "    return img_to_array(load_img(path))\n",
    "\n",
    "\n",
    "def resize(image, size):\n",
    "    return tf.image.resize(image, size)\n",
    "\n",
    "\n",
    "def aggressive_cropping(image, copies, crop_window, resize_smallest_side, output_shape):\n",
    "    global img, resized_copies, crops\n",
    "\n",
    "    if isinstance(resize_smallest_side, int):\n",
    "        img = resize(image, (resize_smallest_side, resize_smallest_side))\n",
    "\n",
    "    if isinstance(resize_smallest_side, (list, tuple)):\n",
    "        resized_copies = [tf.image.resize(image, (size, size)) for size in resize_smallest_side]\n",
    "\n",
    "    if isinstance(crop_window, int):\n",
    "        if isinstance(resize_smallest_side, int):\n",
    "            crops = [tf.image.random_crop(img, crop_window) for _ in range(copies)]\n",
    "        elif isinstance(resize_smallest_side, (list, tuple)):\n",
    "            crops = [tf.image.random_crop(img_, crop_window) for _ in range(copies) for img_ in\n",
    "                     resized_copies]\n",
    "\n",
    "    elif isinstance(crop_window, (list, tuple)):\n",
    "        if isinstance(resize_smallest_side, int):\n",
    "            crops = [tf.image.random_crop(img, crop_window) for _ in range(copies)]\n",
    "        elif isinstance(resize_smallest_side, (list, tuple)):\n",
    "            crops = [tf.image.random_crop(img_, crop_window) for _ in range(copies) for img_ in resized_copies]\n",
    "\n",
    "    return [resize(crop_img, output_shape) for crop_img in crops]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T09:18:35.694206Z",
     "iopub.execute_input": "2022-04-21T09:18:35.694608Z",
     "iopub.status.idle": "2022-04-21T09:18:35.711092Z",
     "shell.execute_reply.started": "2022-04-21T09:18:35.694562Z",
     "shell.execute_reply": "2022-04-21T09:18:35.710095Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "os.mkdir('dataset_1')\n",
    "os.mkdir('dataset_2')\n",
    "\n",
    "os.mkdir('dataset_1/train')\n",
    "os.mkdir('dataset_1/valid')\n",
    "\n",
    "os.mkdir('dataset_2/train')\n",
    "os.mkdir('dataset_2/valid')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T09:18:35.712420Z",
     "iopub.execute_input": "2022-04-21T09:18:35.712930Z",
     "iopub.status.idle": "2022-04-21T09:18:35.732568Z",
     "shell.execute_reply.started": "2022-04-21T09:18:35.712887Z",
     "shell.execute_reply": "2022-04-21T09:18:35.731310Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for k, sub_class in enumerate(os.listdir('../input/100-bird-species/valid/')):\n",
    "    sub_path = os.path.join('../input/100-bird-species/valid/', sub_class)\n",
    "    dst = os.path.join('dataset_1/valid/', sub_class)\n",
    "\n",
    "    if not os.path.isdir(dst):\n",
    "        os.mkdir(dst)\n",
    "\n",
    "    for file in os.listdir(sub_path):\n",
    "        file_path = os.path.join(sub_path, file)\n",
    "        img_arr = load_img_to_array(file_path)\n",
    "        copies = aggressive_cropping(img_arr, 3, [128, 128, 3], [224, 224], [32, 32])\n",
    "\n",
    "        for i, copy in enumerate(copies):\n",
    "            array_to_img(copy).save(os.path.join(dst, f'c_{i}_{file}'))\n",
    "\n",
    "        array_to_img(resize(img_arr, (32, 32))).save(os.path.join(dst, file))\n",
    "\n",
    "    print(k + 1, end='\\r')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T09:48:40.220157Z",
     "iopub.execute_input": "2022-04-21T09:48:40.220456Z",
     "iopub.status.idle": "2022-04-21T09:49:25.706602Z",
     "shell.execute_reply.started": "2022-04-21T09:48:40.220423Z",
     "shell.execute_reply": "2022-04-21T09:49:25.705638Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for k, sub_class in enumerate(os.listdir('../input/intel-image-classification/seg_test/seg_test/')):\n",
    "    sub_path = os.path.join('../input/intel-image-classification/seg_test/seg_test/', sub_class)\n",
    "    dst = os.path.join('dataset_2/valid/', sub_class)\n",
    "\n",
    "    if not os.path.isdir(dst):\n",
    "        os.mkdir(dst)\n",
    "\n",
    "    for file in os.listdir(sub_path):\n",
    "        file_path = os.path.join(sub_path, file)\n",
    "        img_arr = load_img_to_array(file_path)\n",
    "        copies = aggressive_cropping(img_arr, 3, [128, 128, 3], [224, 224], [32, 32])\n",
    "\n",
    "        for i, copy in enumerate(copies):\n",
    "            array_to_img(copy).save(os.path.join(dst, f'c_{i}_{file}'))\n",
    "\n",
    "        array_to_img(resize(img_arr, (32, 32))).save(os.path.join(dst, file))\n",
    "\n",
    "    print(k + 1, end='\\r')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-04-21T09:50:37.981801Z",
     "iopub.execute_input": "2022-04-21T09:50:37.982116Z",
     "iopub.status.idle": "2022-04-21T09:51:46.307293Z",
     "shell.execute_reply.started": "2022-04-21T09:50:37.982077Z",
     "shell.execute_reply": "2022-04-21T09:51:46.306364Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_generator = ImageDataGenerator(rescale=1 / 255.,\n",
    "                                     horizontal_flip=True)\n",
    "\n",
    "valid_generator = ImageDataGenerator(rescale=1 / 255.,\n",
    "                                     horizontal_flip=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T09:51:51.658284Z",
     "iopub.execute_input": "2022-04-21T09:51:51.658628Z",
     "iopub.status.idle": "2022-04-21T09:51:51.664389Z",
     "shell.execute_reply.started": "2022-04-21T09:51:51.658596Z",
     "shell.execute_reply": "2022-04-21T09:51:51.663315Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_set = train_generator.flow_from_directory(directory='dataset_1/train',\n",
    "                                                target_size=(32, 32),\n",
    "                                                batch_size=128,\n",
    "                                                  subset='training')\n",
    "\n",
    "val_set = valid_generator.flow_from_directory(directory='dataset_1/valid',\n",
    "                                              target_size=(32, 32),\n",
    "                                              batch_size=128)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T09:52:04.220849Z",
     "iopub.execute_input": "2022-04-21T09:52:04.221304Z",
     "iopub.status.idle": "2022-04-21T09:52:33.219619Z",
     "shell.execute_reply.started": "2022-04-21T09:52:04.221257Z",
     "shell.execute_reply": "2022-04-21T09:52:33.218579Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_set_1 = train_generator.flow_from_directory(directory='dataset_2/train',\n",
    "                                                  target_size=(32, 32),\n",
    "                                                  batch_size=128,\n",
    "                                                 color_mode='grayscale')\n",
    "\n",
    "val_set_1 = valid_generator.flow_from_directory(directory='dataset_2/valid',\n",
    "                                                target_size=(32, 32),\n",
    "                                                batch_size=128,\n",
    "                                               color_mode='grayscale')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T09:52:33.221517Z",
     "iopub.execute_input": "2022-04-21T09:52:33.222553Z",
     "iopub.status.idle": "2022-04-21T09:52:41.005458Z",
     "shell.execute_reply.started": "2022-04-21T09:52:33.222504Z",
     "shell.execute_reply": "2022-04-21T09:52:41.004390Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "fixed_data = []\n",
    "\n",
    "for i, file in enumerate(os.listdir('../input/intel-image-classification/seg_pred/seg_pred/')):\n",
    "    if i == 64:\n",
    "        break\n",
    "    else:\n",
    "        img_arr = img_to_array(load_img(os.path.join('../input/intel-image-classification/seg_pred/seg_pred',file),\n",
    "                                        color_mode='grayscale',\n",
    "                                       target_size=(32,32)))\n",
    "        fixed_data.append(img_arr)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T09:54:27.583836Z",
     "iopub.execute_input": "2022-04-21T09:54:27.584330Z",
     "iopub.status.idle": "2022-04-21T09:54:27.721821Z",
     "shell.execute_reply.started": "2022-04-21T09:54:27.584281Z",
     "shell.execute_reply": "2022-04-21T09:54:27.720774Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ***Models***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "init = HeNormal()\n",
    "classes = [6, 400]\n",
    "act_name = ['relu', 'lrelu', 'srelu', 'elu']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-04-21T09:52:54.534628Z",
     "iopub.execute_input": "2022-04-21T09:52:54.535049Z",
     "iopub.status.idle": "2022-04-21T09:52:54.546721Z",
     "shell.execute_reply.started": "2022-04-21T09:52:54.535005Z",
     "shell.execute_reply": "2022-04-21T09:52:54.545528Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 3500:\n",
    "        return 0.01\n",
    "    elif 3500 < epoch < 8500:\n",
    "        return 0.005\n",
    "    elif 8500 < epoch < 13500:\n",
    "        return 0.0005\n",
    "    elif 13500 < epoch < 16500:\n",
    "        return 0.00005\n",
    "\n",
    "\n",
    "class AvgUnitActivations(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, subset):\n",
    "        self.train_subset = subset\n",
    "        self.activations = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        out_layer = self.model.layers[-2]\n",
    "        activations = Model(self.model.input, out_layer.output)(self.train_subset)\n",
    "        self.activations.append(activations)\n",
    "\n",
    "\n",
    "lr_schedule = LearningRateScheduler(scheduler)\n",
    "avg_activation = AvgUnitActivations(tf.Variable(fixed_data, shape=(64,32,32,1)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-04-21T09:54:32.701168Z",
     "iopub.execute_input": "2022-04-21T09:54:32.701480Z",
     "iopub.status.idle": "2022-04-21T09:54:32.820415Z",
     "shell.execute_reply.started": "2022-04-21T09:54:32.701434Z",
     "shell.execute_reply": "2022-04-21T09:54:32.819393Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "activation_learn_history_0 = {}\n",
    "\n",
    "for act in act_name:\n",
    "    learning_history = []\n",
    "\n",
    "    for _ in range(5):\n",
    "        model1 = Sequential()\n",
    "\n",
    "        model1.add(InputLayer((32, 32, 1)))\n",
    "        model1.add(Flatten())\n",
    "\n",
    "        for _ in range(8):\n",
    "            model1.add(Dense(units=128, kernel_initializer=init))\n",
    "\n",
    "            if act == 'relu':\n",
    "                model1.add(ReLU())\n",
    "            elif act == 'lrelu':\n",
    "                model1.add(LeakyReLU(alpha=0.1))\n",
    "            elif act == 'elu':\n",
    "                model1.add(ELU())\n",
    "\n",
    "        model1.add(Dense(units=classes[0], activation='softmax'))\n",
    "\n",
    "        model1.compile(optimizer=SGD(learning_rate=0.01),\n",
    "                       loss=categorical_crossentropy,\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "        history = model1.fit(train_set_1,\n",
    "                             validation_data=val_set_1,\n",
    "                             steps_per_epoch=256,\n",
    "                             validation_steps=128,\n",
    "                             epochs=100,\n",
    "                             callbacks=[avg_activation])\n",
    "\n",
    "        learning_history.append(history.history)\n",
    "\n",
    "    activation_learn_history_0[act] = learning_history\n",
    "\n",
    "with open('result_0.json', 'w') as file:\n",
    "    json.dump(activation_learn_history_0, file)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-21T09:57:42.304456Z",
     "iopub.execute_input": "2022-04-21T09:57:42.304759Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "activation_learn_history_1 = {}\n",
    "\n",
    "for act in act_name:\n",
    "    learning_history = []\n",
    "\n",
    "    for l in range(3):\n",
    "        model2 = Sequential()\n",
    "\n",
    "        model2.add(InputLayer((64, 64, 3)))\n",
    "        model2.add(Conv2D(filters=192, kernel_size=(5, 5), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005), padding='same'))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'srelu':\n",
    "            model2.add(ReLU(negative_slope=-1))\n",
    "        elif act == 'elu':\n",
    "            model2.add(ELU())\n",
    "\n",
    "        model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2.add(Dropout(rate=0.0))\n",
    "\n",
    "        model2.add(Conv2D(filters=192, kernel_size=(1, 1), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005), padding='same'))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'srelu':\n",
    "            model2.add(ReLU(max_value=-1))\n",
    "        elif act == 'elu':\n",
    "            model2.add(ELU())\n",
    "\n",
    "        model2.add(Conv2D(filters=240, kernel_size=(3, 3), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005), padding='same'))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'srelu':\n",
    "            model2.add(ReLU(negative_slope=-1))\n",
    "        elif act == 'elu':\n",
    "            model2.add(ELU())\n",
    "\n",
    "        model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2.add(Dropout(rate=0.1))\n",
    "\n",
    "        model2.add(Conv2D(filters=240, kernel_size=(1, 1), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005), padding='same'))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'srelu':\n",
    "            model2.add(ReLU(negative_slope=-1))\n",
    "        elif act == 'elu':\n",
    "            model2.add(ELU())\n",
    "\n",
    "        model2.add(Conv2D(filters=260, kernel_size=(2, 2), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005), padding='same'))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'srelu':\n",
    "            model2.add(ReLU(negative_slope=-1))\n",
    "        elif act == 'elu':\n",
    "            model2.add(ELU())\n",
    "\n",
    "        model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2.add(Dropout(rate=0.2))\n",
    "\n",
    "        model2.add(Conv2D(filters=260, kernel_size=(1, 1), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005), padding='same'))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'srelu':\n",
    "            model2.add(ReLU(negative_slope=-1))\n",
    "        elif act == 'elu':\n",
    "            model2.add(ELU())\n",
    "\n",
    "        model2.add(Conv2D(filters=280, kernel_size=(2, 2), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005), padding='same'))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'srelu':\n",
    "            model2.add(ReLU(negative_slope=-1))\n",
    "        elif act == 'elu':\n",
    "            model2.add(ELU())\n",
    "\n",
    "        model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2.add(Dropout(rate=0.3))\n",
    "\n",
    "        model2.add(Conv2D(filters=280, kernel_size=(1, 1), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005), padding='same'))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'srelu':\n",
    "            model2.add(ReLU(negative_slope=-1))\n",
    "        elif act == 'elu':\n",
    "            model2.add(ELU())\n",
    "\n",
    "        model2.add(Conv2D(filters=300, kernel_size=(2, 2), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005), padding='same'))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'srelu':\n",
    "            model2.add(ReLU(negative_slope=-1))\n",
    "        elif act == 'elu':\n",
    "            model2.add(ELU())\n",
    "\n",
    "        model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2.add(Dropout(rate=0.4))\n",
    "\n",
    "        model2.add(Conv2D(filters=300, kernel_size=(1, 1), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005), padding='same'))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'srelu':\n",
    "            model2.add(ReLU(negative_slope=-1))\n",
    "        elif act == 'elu':\n",
    "            model2.add(ELU())\n",
    "\n",
    "        model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2.add(Dropout(rate=0.5))\n",
    "\n",
    "        model2.add(Flatten())\n",
    "        model2.add(Dense(classes[1], activation='softmax'))\n",
    "\n",
    "        model2.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9),\n",
    "                       loss=categorical_crossentropy,\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "        print(f'{l+1}/3')\n",
    "\n",
    "        history = model2.fit(train_set,\n",
    "                             validation_data=val_set,\n",
    "                             steps_per_epoch=256,\n",
    "                             validation_steps=128,\n",
    "                             epochs=1650,\n",
    "                             batch_size=128,\n",
    "                             callbacks=[lr_schedule])\n",
    "\n",
    "        learning_history.append(history.history)\n",
    "\n",
    "    activation_learn_history_1[act] = learning_history\n",
    "\n",
    "with open('result_1.json', 'w') as file:\n",
    "    json.dump(activation_learn_history_1, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "activation_learn_history_2 = {}\n",
    "\n",
    "for act in act_name:\n",
    "    learning_history = []\n",
    "\n",
    "    for _ in range(5):\n",
    "        model2_bn = Sequential()\n",
    "\n",
    "        model2_bn.add(InputLayer((32, 32, 3)))\n",
    "        model2_bn.add(\n",
    "            Conv2D(filters=192, kernel_size=(5, 5), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005)))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2_bn.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2_bn.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'srelu':\n",
    "            model2_bn.add(ReLU(max_value=-1))\n",
    "        elif act == 'elu':\n",
    "            model2_bn.add(ELU())\n",
    "\n",
    "        model2_bn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2_bn.add(Dropout(rate=0.0))\n",
    "        model2_bn.add(BatchNormalization())\n",
    "\n",
    "        model2_bn.add(\n",
    "            Conv2D(filters=192, kernel_size=(1, 1), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005)))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2_bn.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2_bn.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'srelu':\n",
    "            model2_bn.add(ReLU(negative_slope=-1))\n",
    "        elif act == 'elu':\n",
    "            model2_bn.add(ELU())\n",
    "\n",
    "        model2_bn.add(\n",
    "            Conv2D(filters=240, kernel_size=(3, 3), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005)))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2_bn.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2_bn.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'srelu':\n",
    "            model2_bn.add(ReLU(negative_slope=-1))\n",
    "        elif act == 'elu':\n",
    "            model2_bn.add(ELU())\n",
    "\n",
    "        model2_bn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2_bn.add(Dropout(rate=0.1))\n",
    "        model2_bn.add(BatchNormalization())\n",
    "\n",
    "        model2_bn.add(\n",
    "            Conv2D(filters=240, kernel_size=(1, 1), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005)))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2_bn.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2_bn.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'srelu':\n",
    "            model2_bn.add(ReLU(negative_slope=-1))\n",
    "        elif act == 'elu':\n",
    "            model2_bn.add(ELU())\n",
    "\n",
    "        model2_bn.add(\n",
    "            Conv2D(filters=260, kernel_size=(2, 2), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005)))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2_bn.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2_bn.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'srelu':\n",
    "            model2_bn.add(ReLU(negative_slope=-1))\n",
    "        elif act == 'elu':\n",
    "            model2_bn.add(ELU())\n",
    "\n",
    "        model2_bn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2_bn.add(Dropout(rate=0.2))\n",
    "        model2_bn.add(BatchNormalization())\n",
    "\n",
    "        model2_bn.add(\n",
    "            Conv2D(filters=260, kernel_size=(1, 1), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005)))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2_bn.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2_bn.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'srelu':\n",
    "            model2_bn.add(ReLU(negative_slope=-1))\n",
    "        elif act == 'elu':\n",
    "            model2_bn.add(ELU())\n",
    "\n",
    "        model2_bn.add(\n",
    "            Conv2D(filters=280, kernel_size=(2, 2), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005)))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2_bn.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2_bn.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'srelu':\n",
    "            model2_bn.add(ReLU(negative_slope=-1))\n",
    "        elif act == 'elu':\n",
    "            model2_bn.add(ELU())\n",
    "\n",
    "        model2_bn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2_bn.add(Dropout(rate=0.3))\n",
    "        model2_bn.add(BatchNormalization())\n",
    "\n",
    "        model2_bn.add(\n",
    "            Conv2D(filters=280, kernel_size=(1, 1), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005)))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2_bn.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2_bn.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'srelu':\n",
    "            model2_bn.add(ReLU(negative_slope=-1))\n",
    "        elif act == 'elu':\n",
    "            model2_bn.add(ELU())\n",
    "\n",
    "        model2_bn.add(\n",
    "            Conv2D(filters=300, kernel_size=(2, 2), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005)))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2_bn.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2_bn.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'srelu':\n",
    "            model2_bn.add(ReLU(negative_slope=-1))\n",
    "        elif act == 'elu':\n",
    "            model2_bn.add(ELU())\n",
    "\n",
    "        model2_bn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2_bn.add(Dropout(rate=0.4))\n",
    "        model2_bn.add(BatchNormalization())\n",
    "\n",
    "        model2_bn.add(\n",
    "            Conv2D(filters=300, kernel_size=(1, 1), kernel_initializer=init, kernel_regularizer=l2(l2=0.0005)))\n",
    "\n",
    "        if act == 'relu':\n",
    "            model2_bn.add(ReLU())\n",
    "        elif act == 'lrelu':\n",
    "            model2_bn.add(LeakyReLU(alpha=0.1))\n",
    "        elif act == 'srelu':\n",
    "            model2_bn.add(ReLU(negative_slope=-1))\n",
    "        elif act == 'elu':\n",
    "            model2_bn.add(ELU())\n",
    "\n",
    "        model2_bn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model2_bn.add(Dropout(rate=0.5))\n",
    "        model2_bn.add(BatchNormalization())\n",
    "\n",
    "        model2_bn.add(Flatten())\n",
    "        model2_bn.add(Dense(filters=classes[1], activation='softmax'))\n",
    "\n",
    "        model2_bn.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9),\n",
    "                          loss=categorical_crossentropy,\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "        history = model2_bn.fit(train_set,\n",
    "                                validation_data=val_set,\n",
    "                                steps_per_epoch=256,\n",
    "                                validation_steps=128,\n",
    "                                epochs=16500,\n",
    "                                batch_size=128,\n",
    "                                callbacks=[lr_schedule])\n",
    "\n",
    "        learning_history.append(history.history)\n",
    "\n",
    "    activation_learn_history_2[act] = learning_history\n",
    "\n",
    "with open('result_2.json', 'w') as file:\n",
    "    json.dump(activation_learn_history_2, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}