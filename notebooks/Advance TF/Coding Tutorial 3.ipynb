{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence modelling \n",
    "\n",
    "## Coding tutorials\n",
    " #### [1.  The IMDb dataset](#coding_tutorial_1)\n",
    " #### [2. Padding and masking sequence data](#coding_tutorial_2)\n",
    " #### [3. The Embedding layer](#coding_tutorial_3)\n",
    " #### [4. The Embedding Projector](#coding_tutorial_4)\n",
    " #### [5. Recurrent neural network layers](#coding_tutorial_5)\n",
    " #### [6. Stacked RNNs and the Bidirectional wrapper](#coding_tutorial_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_1\"></a>\n",
    "## The IMDb Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the IMDB review sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import imdb\n",
    "\n",
    "import keras.datasets.imdb as imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and assign the data set using load_data()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the type of the data\n",
    "\n",
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(25000,)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the shape of the data\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[1,\n 14,\n 22,\n 16,\n 43,\n 530,\n 973,\n 1622,\n 1385,\n 65,\n 458,\n 4468,\n 66,\n 3941,\n 4,\n 173,\n 36,\n 256,\n 5,\n 25,\n 100,\n 43,\n 838,\n 112,\n 50,\n 670,\n 22665,\n 9,\n 35,\n 480,\n 284,\n 5,\n 150,\n 4,\n 172,\n 112,\n 167,\n 21631,\n 336,\n 385,\n 39,\n 4,\n 172,\n 4536,\n 1111,\n 17,\n 546,\n 38,\n 13,\n 447,\n 4,\n 192,\n 50,\n 16,\n 6,\n 147,\n 2025,\n 19,\n 14,\n 22,\n 4,\n 1920,\n 4613,\n 469,\n 4,\n 22,\n 71,\n 87,\n 12,\n 16,\n 43,\n 530,\n 38,\n 76,\n 15,\n 13,\n 1247,\n 4,\n 22,\n 17,\n 515,\n 17,\n 12,\n 16,\n 626,\n 18,\n 19193,\n 5,\n 62,\n 386,\n 12,\n 8,\n 316,\n 8,\n 106,\n 5,\n 4,\n 2223,\n 5244,\n 16,\n 480,\n 66,\n 3785,\n 33,\n 4,\n 130,\n 12,\n 16,\n 38,\n 619,\n 5,\n 25,\n 124,\n 51,\n 36,\n 135,\n 48,\n 25,\n 1415,\n 33,\n 6,\n 22,\n 12,\n 215,\n 28,\n 77,\n 52,\n 5,\n 14,\n 407,\n 16,\n 82,\n 10311,\n 8,\n 4,\n 107,\n 117,\n 5952,\n 15,\n 256,\n 4,\n 31050,\n 7,\n 3766,\n 5,\n 723,\n 36,\n 71,\n 43,\n 530,\n 476,\n 26,\n 400,\n 317,\n 46,\n 7,\n 4,\n 12118,\n 1029,\n 13,\n 104,\n 88,\n 4,\n 381,\n 15,\n 297,\n 98,\n 32,\n 2071,\n 56,\n 26,\n 141,\n 6,\n 194,\n 7486,\n 18,\n 4,\n 226,\n 22,\n 21,\n 134,\n 476,\n 26,\n 480,\n 5,\n 144,\n 30,\n 5535,\n 18,\n 51,\n 36,\n 28,\n 224,\n 92,\n 25,\n 104,\n 4,\n 226,\n 65,\n 16,\n 38,\n 1334,\n 88,\n 12,\n 16,\n 283,\n 5,\n 16,\n 4472,\n 113,\n 103,\n 32,\n 15,\n 16,\n 5345,\n 19,\n 178,\n 32]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first dataset element input\n",
    "# Notice encoding\n",
    "\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first dataset element output\n",
    "\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset with different options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with defaults\n",
    "# ~/.keras/dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "((array([list([1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]),\n         list([1, 194, 2, 194, 2, 78, 228, 5, 6, 2, 2, 2, 134, 26, 4, 715, 8, 118, 2, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 2, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2, 2, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 2, 4, 118, 9, 4, 130, 2, 19, 4, 2, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 2, 2, 398, 4, 2, 26, 2, 5, 163, 11, 2, 2, 4, 2, 9, 194, 775, 7, 2, 2, 349, 2, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 2, 98, 5, 4, 228, 9, 43, 2, 2, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 2, 228, 2, 5, 2, 656, 245, 2, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 2, 14, 9, 6, 371, 78, 22, 625, 64, 2, 9, 8, 168, 145, 23, 4, 2, 15, 16, 4, 2, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2, 311, 12, 16, 2, 33, 75, 43, 2, 296, 4, 86, 320, 35, 534, 19, 263, 2, 2, 4, 2, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 2, 43, 645, 662, 8, 257, 85, 2, 42, 2, 2, 83, 68, 2, 15, 36, 165, 2, 278, 36, 69, 2, 780, 8, 106, 14, 2, 2, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2, 51, 9, 170, 23, 595, 116, 595, 2, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n         ...,\n         list([1, 11, 6, 230, 245, 2, 9, 6, 2, 446, 2, 45, 2, 84, 2, 2, 21, 4, 912, 84, 2, 325, 725, 134, 2, 2, 84, 5, 36, 28, 57, 2, 21, 8, 140, 8, 703, 5, 2, 84, 56, 18, 2, 14, 9, 31, 7, 4, 2, 2, 2, 2, 2, 18, 6, 20, 207, 110, 563, 12, 8, 2, 2, 8, 97, 6, 20, 53, 2, 74, 4, 460, 364, 2, 29, 270, 11, 960, 108, 45, 40, 29, 2, 395, 11, 6, 2, 500, 7, 2, 89, 364, 70, 29, 140, 4, 64, 2, 11, 4, 2, 26, 178, 4, 529, 443, 2, 5, 27, 710, 117, 2, 2, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 2, 2, 10, 10, 288, 2, 2, 34, 2, 2, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2, 234, 2, 2, 7, 496, 4, 139, 929, 2, 2, 2, 5, 2, 18, 4, 2, 2, 250, 11, 2, 2, 4, 2, 2, 747, 2, 372, 2, 2, 541, 2, 7, 4, 59, 2, 4, 2, 2]),\n         list([1, 2, 2, 69, 72, 2, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 2, 2, 45, 58, 2, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 2, 51, 575, 32, 61, 369, 71, 66, 770, 12, 2, 75, 100, 2, 8, 4, 105, 37, 69, 147, 712, 75, 2, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 2, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 2, 40, 319, 2, 112, 2, 11, 2, 121, 25, 70, 2, 4, 719, 2, 13, 18, 31, 62, 40, 8, 2, 4, 2, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23]),\n         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 2, 270, 2, 5, 2, 2, 732, 2, 101, 405, 39, 14, 2, 4, 2, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 2, 2, 9, 24, 6, 78, 2, 17, 2, 2, 21, 27, 2, 2, 5, 2, 2, 92, 2, 4, 2, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 2, 2, 9, 6, 66, 78, 2, 4, 631, 2, 5, 2, 272, 191, 2, 6, 2, 8, 2, 2, 2, 544, 5, 383, 2, 848, 2, 2, 497, 2, 8, 2, 2, 2, 21, 60, 27, 239, 9, 43, 2, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 2, 72, 7, 51, 6, 2, 22, 4, 204, 131, 9])],\n        dtype=object),\n  array([1, 0, 0, ..., 0, 1, 0], dtype=int64)),\n (array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 2, 2, 5, 4, 360, 7, 4, 177, 2, 394, 354, 4, 123, 9, 2, 2, 2, 10, 10, 13, 92, 124, 89, 488, 2, 100, 28, 2, 14, 31, 23, 27, 2, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 2, 38, 32, 25, 2, 451, 202, 14, 6, 717]),\n         list([1, 14, 22, 2, 6, 176, 7, 2, 88, 12, 2, 23, 2, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 2, 2, 4, 2, 2, 109, 2, 21, 4, 22, 2, 8, 6, 2, 2, 10, 10, 4, 105, 987, 35, 841, 2, 19, 861, 2, 5, 2, 2, 45, 55, 221, 15, 670, 2, 526, 14, 2, 4, 405, 5, 2, 7, 27, 85, 108, 131, 4, 2, 2, 2, 405, 9, 2, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 2, 2, 45, 407, 31, 7, 41, 2, 105, 21, 59, 299, 12, 38, 950, 5, 2, 15, 45, 629, 488, 2, 127, 6, 52, 292, 17, 4, 2, 185, 132, 2, 2, 2, 488, 2, 47, 6, 392, 173, 4, 2, 2, 270, 2, 4, 2, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2, 7, 2, 861, 2, 5, 2, 30, 2, 2, 56, 4, 841, 5, 990, 692, 8, 4, 2, 398, 229, 10, 10, 13, 2, 670, 2, 14, 9, 31, 7, 27, 111, 108, 15, 2, 19, 2, 2, 875, 551, 14, 22, 9, 2, 21, 45, 2, 5, 45, 252, 8, 2, 6, 565, 921, 2, 39, 4, 529, 48, 25, 181, 8, 67, 35, 2, 22, 49, 238, 60, 135, 2, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 2, 25, 203, 28, 8, 818, 12, 125, 4, 2]),\n         list([1, 111, 748, 2, 2, 2, 2, 4, 87, 2, 2, 7, 31, 318, 2, 7, 4, 498, 2, 748, 63, 29, 2, 220, 686, 2, 5, 17, 12, 575, 220, 2, 17, 6, 185, 132, 2, 16, 53, 928, 11, 2, 74, 4, 438, 21, 27, 2, 589, 8, 22, 107, 2, 2, 997, 2, 8, 35, 2, 2, 11, 22, 231, 54, 29, 2, 29, 100, 2, 2, 34, 2, 2, 2, 5, 2, 98, 31, 2, 33, 6, 58, 14, 2, 2, 8, 4, 365, 7, 2, 2, 356, 346, 4, 2, 2, 63, 29, 93, 11, 2, 11, 2, 33, 6, 58, 54, 2, 431, 748, 7, 32, 2, 16, 11, 94, 2, 10, 10, 4, 993, 2, 7, 4, 2, 2, 2, 2, 8, 847, 8, 2, 121, 31, 7, 27, 86, 2, 2, 16, 6, 465, 993, 2, 2, 573, 17, 2, 42, 4, 2, 37, 473, 6, 711, 6, 2, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 849, 2, 53, 33, 2, 2, 37, 70, 2, 4, 2, 2, 74, 476, 37, 62, 91, 2, 169, 4, 2, 2, 146, 655, 2, 5, 258, 12, 184, 2, 546, 5, 849, 2, 7, 4, 22, 2, 18, 631, 2, 797, 7, 4, 2, 71, 348, 425, 2, 2, 19, 2, 5, 2, 11, 661, 8, 339, 2, 4, 2, 2, 7, 4, 2, 10, 10, 263, 787, 9, 270, 11, 6, 2, 4, 2, 2, 121, 4, 2, 26, 2, 19, 68, 2, 5, 28, 446, 6, 318, 2, 8, 67, 51, 36, 70, 81, 8, 2, 2, 36, 2, 8, 2, 2, 18, 6, 711, 4, 2, 26, 2, 2, 11, 14, 636, 720, 12, 426, 28, 77, 776, 8, 97, 38, 111, 2, 2, 168, 2, 2, 137, 2, 18, 27, 173, 9, 2, 17, 6, 2, 428, 2, 232, 11, 4, 2, 37, 272, 40, 2, 247, 30, 656, 6, 2, 54, 2, 2, 98, 6, 2, 40, 558, 37, 2, 98, 4, 2, 2, 15, 14, 9, 57, 2, 5, 2, 6, 275, 711, 2, 2, 2, 98, 6, 2, 10, 10, 2, 19, 14, 2, 267, 162, 711, 37, 2, 752, 98, 4, 2, 2, 90, 19, 6, 2, 7, 2, 2, 2, 4, 2, 2, 930, 8, 508, 90, 4, 2, 8, 4, 2, 17, 2, 2, 2, 4, 2, 8, 2, 189, 4, 2, 2, 2, 4, 2, 5, 95, 271, 23, 6, 2, 2, 2, 2, 33, 2, 6, 425, 2, 2, 2, 2, 7, 4, 2, 2, 469, 4, 2, 54, 4, 150, 2, 2, 280, 53, 2, 2, 18, 339, 29, 2, 27, 2, 5, 2, 68, 2, 19, 2, 2, 4, 2, 7, 263, 65, 2, 34, 6, 2, 2, 43, 159, 29, 9, 2, 9, 387, 73, 195, 584, 10, 10, 2, 4, 58, 810, 54, 14, 2, 117, 22, 16, 93, 5, 2, 4, 192, 15, 12, 16, 93, 34, 6, 2, 2, 33, 4, 2, 7, 15, 2, 2, 2, 325, 12, 62, 30, 776, 8, 67, 14, 17, 6, 2, 44, 148, 687, 2, 203, 42, 203, 24, 28, 69, 2, 2, 11, 330, 54, 29, 93, 2, 21, 845, 2, 27, 2, 7, 819, 4, 22, 2, 17, 6, 2, 787, 7, 2, 2, 2, 100, 30, 4, 2, 2, 2, 2, 42, 2, 11, 4, 2, 42, 101, 704, 7, 101, 999, 15, 2, 94, 2, 180, 5, 9, 2, 34, 2, 45, 6, 2, 22, 60, 6, 2, 31, 11, 94, 2, 96, 21, 94, 749, 9, 57, 975]),\n         ...,\n         list([1, 13, 2, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 2, 21, 45, 184, 78, 4, 2, 910, 769, 2, 2, 395, 2, 5, 2, 11, 119, 2, 89, 2, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2, 284, 2, 2, 37, 315, 4, 226, 20, 272, 2, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 2]),\n         list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 2, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2, 7, 743, 46, 2, 9, 2, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 2, 5, 9, 2, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 2, 92, 124, 51, 45, 2, 71, 536, 13, 520, 14, 20, 6, 2, 7, 470]),\n         list([1, 6, 52, 2, 430, 22, 9, 220, 2, 8, 28, 2, 519, 2, 6, 769, 15, 47, 6, 2, 2, 8, 114, 5, 33, 222, 31, 55, 184, 704, 2, 2, 19, 346, 2, 5, 6, 364, 350, 4, 184, 2, 9, 133, 2, 11, 2, 2, 21, 4, 2, 2, 570, 50, 2, 2, 9, 6, 2, 17, 6, 2, 2, 21, 17, 6, 2, 232, 2, 2, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 2, 19, 4, 78, 173, 7, 27, 2, 2, 2, 718, 2, 9, 6, 2, 17, 210, 5, 2, 2, 47, 77, 395, 14, 172, 173, 18, 2, 2, 2, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 2, 53, 40, 35, 390, 7, 11, 4, 2, 7, 4, 314, 74, 6, 792, 22, 2, 19, 714, 727, 2, 382, 4, 91, 2, 439, 19, 14, 20, 9, 2, 2, 2, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2, 2])],\n        dtype=object),\n  array([0, 1, 1, ..., 0, 0, 0], dtype=int64)))"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limit the vocabulary to the top 500 words using num_words\n",
    "\n",
    "imdb.load_data(num_words=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "((array([list([2, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 2, 173, 36, 256, 2, 25, 100, 43, 838, 112, 50, 670, 2, 2, 35, 480, 284, 2, 150, 2, 172, 112, 167, 2, 336, 385, 39, 2, 172, 2, 2, 17, 546, 38, 13, 447, 2, 192, 50, 16, 2, 147, 2, 19, 14, 22, 2, 2, 2, 469, 2, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 2, 22, 17, 515, 17, 12, 16, 626, 18, 2, 2, 62, 386, 12, 2, 316, 2, 106, 2, 2, 2, 2, 16, 480, 66, 2, 33, 2, 130, 12, 16, 38, 619, 2, 25, 124, 51, 36, 135, 48, 25, 2, 33, 2, 22, 12, 215, 28, 77, 52, 2, 14, 407, 16, 82, 2, 2, 2, 107, 117, 2, 15, 256, 2, 2, 2, 2, 2, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 2, 2, 2, 2, 13, 104, 88, 2, 381, 15, 297, 98, 32, 2, 56, 26, 141, 2, 194, 2, 18, 2, 226, 22, 21, 134, 476, 26, 480, 2, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 2, 226, 65, 16, 38, 2, 88, 12, 16, 283, 2, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]),\n         list([2, 194, 2, 194, 2, 78, 228, 2, 2, 2, 2, 2, 134, 26, 2, 715, 2, 118, 2, 14, 394, 20, 13, 119, 954, 189, 102, 2, 207, 110, 2, 21, 14, 69, 188, 2, 30, 23, 2, 2, 249, 126, 93, 2, 114, 2, 2, 2, 2, 647, 2, 116, 2, 35, 2, 2, 229, 2, 340, 2, 2, 118, 2, 2, 130, 2, 19, 2, 2, 2, 89, 29, 952, 46, 37, 2, 455, 2, 45, 43, 38, 2, 2, 398, 2, 2, 26, 2, 2, 163, 11, 2, 2, 2, 2, 2, 194, 775, 2, 2, 2, 349, 2, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 2, 98, 2, 2, 228, 2, 43, 2, 2, 15, 299, 120, 2, 120, 174, 11, 220, 175, 136, 50, 2, 2, 228, 2, 2, 2, 656, 245, 2, 2, 2, 2, 131, 152, 491, 18, 2, 32, 2, 2, 14, 2, 2, 371, 78, 22, 625, 64, 2, 2, 2, 168, 145, 23, 2, 2, 15, 16, 2, 2, 2, 28, 2, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n         list([2, 14, 47, 2, 30, 31, 2, 2, 249, 108, 2, 2, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 2, 2, 311, 12, 16, 2, 33, 75, 43, 2, 296, 2, 86, 320, 35, 534, 19, 263, 2, 2, 2, 2, 33, 89, 78, 12, 66, 16, 2, 360, 2, 2, 58, 316, 334, 11, 2, 2, 43, 645, 662, 2, 257, 85, 2, 42, 2, 2, 83, 68, 2, 15, 36, 165, 2, 278, 36, 69, 2, 780, 2, 106, 14, 2, 2, 18, 2, 22, 12, 215, 28, 610, 40, 2, 87, 326, 23, 2, 21, 23, 22, 12, 272, 40, 57, 31, 11, 2, 22, 47, 2, 2, 51, 2, 170, 23, 595, 116, 595, 2, 13, 191, 79, 638, 89, 2, 14, 2, 2, 106, 607, 624, 35, 534, 2, 227, 2, 129, 113]),\n         ...,\n         list([2, 11, 2, 230, 245, 2, 2, 2, 2, 446, 2, 45, 2, 84, 2, 2, 21, 2, 912, 84, 2, 325, 725, 134, 2, 2, 84, 2, 36, 28, 57, 2, 21, 2, 140, 2, 703, 2, 2, 84, 56, 18, 2, 14, 2, 31, 2, 2, 2, 2, 2, 2, 2, 18, 2, 20, 207, 110, 563, 12, 2, 2, 2, 2, 97, 2, 20, 53, 2, 74, 2, 460, 364, 2, 29, 270, 11, 960, 108, 45, 40, 29, 2, 395, 11, 2, 2, 500, 2, 2, 89, 364, 70, 29, 140, 2, 64, 2, 11, 2, 2, 26, 178, 2, 529, 443, 2, 2, 27, 710, 117, 2, 2, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 2, 2, 10, 10, 288, 2, 2, 34, 2, 2, 2, 65, 496, 2, 231, 2, 790, 2, 2, 320, 234, 2, 234, 2, 2, 2, 496, 2, 139, 929, 2, 2, 2, 2, 2, 18, 2, 2, 2, 250, 11, 2, 2, 2, 2, 2, 747, 2, 372, 2, 2, 541, 2, 2, 2, 59, 2, 2, 2, 2]),\n         list([2, 2, 2, 69, 72, 2, 13, 610, 930, 2, 12, 582, 23, 2, 16, 484, 685, 54, 349, 11, 2, 2, 45, 58, 2, 13, 197, 12, 16, 43, 23, 2, 2, 62, 30, 145, 402, 11, 2, 51, 575, 32, 61, 369, 71, 66, 770, 12, 2, 75, 100, 2, 2, 2, 105, 37, 69, 147, 712, 75, 2, 44, 257, 390, 2, 69, 263, 514, 105, 50, 286, 2, 23, 2, 123, 13, 161, 40, 2, 421, 2, 116, 16, 897, 13, 2, 40, 319, 2, 112, 2, 11, 2, 121, 25, 70, 2, 2, 719, 2, 13, 18, 31, 62, 40, 2, 2, 2, 2, 2, 14, 123, 2, 942, 25, 2, 721, 12, 145, 2, 202, 12, 160, 580, 202, 12, 2, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 2, 15, 251, 2, 2, 12, 38, 84, 80, 124, 12, 2, 23]),\n         list([2, 17, 2, 194, 337, 2, 2, 204, 22, 45, 254, 2, 106, 14, 123, 2, 2, 270, 2, 2, 2, 2, 732, 2, 101, 405, 39, 14, 2, 2, 2, 2, 115, 50, 305, 12, 47, 2, 168, 2, 235, 2, 38, 111, 699, 102, 2, 2, 2, 2, 2, 24, 2, 78, 2, 17, 2, 2, 21, 27, 2, 2, 2, 2, 2, 92, 2, 2, 2, 2, 2, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 2, 97, 12, 157, 21, 2, 2, 2, 2, 66, 78, 2, 2, 631, 2, 2, 2, 272, 191, 2, 2, 2, 2, 2, 2, 2, 544, 2, 383, 2, 848, 2, 2, 497, 2, 2, 2, 2, 2, 21, 60, 27, 239, 2, 43, 2, 209, 405, 10, 10, 12, 764, 40, 2, 248, 20, 12, 16, 2, 174, 2, 72, 2, 51, 2, 2, 22, 2, 204, 131, 2])],\n        dtype=object),\n  array([1, 0, 0, ..., 0, 1, 0], dtype=int64)),\n (array([list([2, 591, 202, 14, 31, 2, 717, 10, 10, 2, 2, 2, 2, 360, 2, 2, 177, 2, 394, 354, 2, 123, 2, 2, 2, 2, 10, 10, 13, 92, 124, 89, 488, 2, 100, 28, 2, 14, 31, 23, 27, 2, 29, 220, 468, 2, 124, 14, 286, 170, 2, 157, 46, 2, 27, 239, 16, 179, 2, 38, 32, 25, 2, 451, 202, 14, 2, 717]),\n         list([2, 14, 22, 2, 2, 176, 2, 2, 88, 12, 2, 23, 2, 2, 109, 943, 2, 114, 2, 55, 606, 2, 111, 2, 2, 139, 193, 273, 23, 2, 172, 270, 11, 2, 2, 2, 2, 2, 109, 2, 21, 2, 22, 2, 2, 2, 2, 2, 10, 10, 2, 105, 987, 35, 841, 2, 19, 861, 2, 2, 2, 2, 45, 55, 221, 15, 670, 2, 526, 14, 2, 2, 405, 2, 2, 2, 27, 85, 108, 131, 2, 2, 2, 2, 405, 2, 2, 133, 2, 50, 13, 104, 51, 66, 166, 14, 22, 157, 2, 2, 530, 239, 34, 2, 2, 45, 407, 31, 2, 41, 2, 105, 21, 59, 299, 12, 38, 950, 2, 2, 15, 45, 629, 488, 2, 127, 2, 52, 292, 17, 2, 2, 185, 132, 2, 2, 2, 488, 2, 47, 2, 392, 173, 2, 2, 2, 270, 2, 2, 2, 2, 2, 65, 55, 73, 11, 346, 14, 20, 2, 2, 976, 2, 2, 2, 861, 2, 2, 2, 30, 2, 2, 56, 2, 841, 2, 990, 692, 2, 2, 2, 398, 229, 10, 10, 13, 2, 670, 2, 14, 2, 31, 2, 27, 111, 108, 15, 2, 19, 2, 2, 875, 551, 14, 22, 2, 2, 21, 45, 2, 2, 45, 252, 2, 2, 2, 565, 921, 2, 39, 2, 529, 48, 25, 181, 2, 67, 35, 2, 22, 49, 238, 60, 135, 2, 14, 2, 290, 2, 58, 10, 10, 472, 45, 55, 878, 2, 169, 11, 374, 2, 25, 203, 28, 2, 818, 12, 125, 2, 2]),\n         list([2, 111, 748, 2, 2, 2, 2, 2, 87, 2, 2, 2, 31, 318, 2, 2, 2, 498, 2, 748, 63, 29, 2, 220, 686, 2, 2, 17, 12, 575, 220, 2, 17, 2, 185, 132, 2, 16, 53, 928, 11, 2, 74, 2, 438, 21, 27, 2, 589, 2, 22, 107, 2, 2, 997, 2, 2, 35, 2, 2, 11, 22, 231, 54, 29, 2, 29, 100, 2, 2, 34, 2, 2, 2, 2, 2, 98, 31, 2, 33, 2, 58, 14, 2, 2, 2, 2, 365, 2, 2, 2, 356, 346, 2, 2, 2, 63, 29, 93, 11, 2, 11, 2, 33, 2, 58, 54, 2, 431, 748, 2, 32, 2, 16, 11, 94, 2, 10, 10, 2, 993, 2, 2, 2, 2, 2, 2, 2, 2, 847, 2, 2, 121, 31, 2, 27, 86, 2, 2, 16, 2, 465, 993, 2, 2, 573, 17, 2, 42, 2, 2, 37, 473, 2, 711, 2, 2, 2, 328, 212, 70, 30, 258, 11, 220, 32, 2, 108, 21, 133, 12, 2, 55, 465, 849, 2, 53, 33, 2, 2, 37, 70, 2, 2, 2, 2, 74, 476, 37, 62, 91, 2, 169, 2, 2, 2, 146, 655, 2, 2, 258, 12, 184, 2, 546, 2, 849, 2, 2, 2, 22, 2, 18, 631, 2, 797, 2, 2, 2, 71, 348, 425, 2, 2, 19, 2, 2, 2, 11, 661, 2, 339, 2, 2, 2, 2, 2, 2, 2, 10, 10, 263, 787, 2, 270, 11, 2, 2, 2, 2, 2, 121, 2, 2, 26, 2, 19, 68, 2, 2, 28, 446, 2, 318, 2, 2, 67, 51, 36, 70, 81, 2, 2, 2, 36, 2, 2, 2, 2, 18, 2, 711, 2, 2, 26, 2, 2, 11, 14, 636, 720, 12, 426, 28, 77, 776, 2, 97, 38, 111, 2, 2, 168, 2, 2, 137, 2, 18, 27, 173, 2, 2, 17, 2, 2, 428, 2, 232, 11, 2, 2, 37, 272, 40, 2, 247, 30, 656, 2, 2, 54, 2, 2, 98, 2, 2, 40, 558, 37, 2, 98, 2, 2, 2, 15, 14, 2, 57, 2, 2, 2, 2, 275, 711, 2, 2, 2, 98, 2, 2, 10, 10, 2, 19, 14, 2, 267, 162, 711, 37, 2, 752, 98, 2, 2, 2, 90, 19, 2, 2, 2, 2, 2, 2, 2, 2, 2, 930, 2, 508, 90, 2, 2, 2, 2, 2, 17, 2, 2, 2, 2, 2, 2, 2, 189, 2, 2, 2, 2, 2, 2, 2, 95, 271, 23, 2, 2, 2, 2, 2, 33, 2, 2, 425, 2, 2, 2, 2, 2, 2, 2, 2, 469, 2, 2, 54, 2, 150, 2, 2, 280, 53, 2, 2, 18, 339, 29, 2, 27, 2, 2, 2, 68, 2, 19, 2, 2, 2, 2, 2, 263, 65, 2, 34, 2, 2, 2, 43, 159, 29, 2, 2, 2, 387, 73, 195, 584, 10, 10, 2, 2, 58, 810, 54, 14, 2, 117, 22, 16, 93, 2, 2, 2, 192, 15, 12, 16, 93, 34, 2, 2, 2, 33, 2, 2, 2, 15, 2, 2, 2, 325, 12, 62, 30, 776, 2, 67, 14, 17, 2, 2, 44, 148, 687, 2, 203, 42, 203, 24, 28, 69, 2, 2, 11, 330, 54, 29, 93, 2, 21, 845, 2, 27, 2, 2, 819, 2, 22, 2, 17, 2, 2, 787, 2, 2, 2, 2, 100, 30, 2, 2, 2, 2, 2, 42, 2, 11, 2, 2, 42, 101, 704, 2, 101, 999, 15, 2, 94, 2, 180, 2, 2, 2, 34, 2, 45, 2, 2, 22, 60, 2, 2, 31, 11, 94, 2, 96, 21, 94, 749, 2, 57, 975]),\n         ...,\n         list([2, 13, 2, 15, 2, 135, 14, 2, 35, 32, 46, 394, 20, 62, 30, 2, 21, 45, 184, 78, 2, 2, 910, 769, 2, 2, 395, 2, 2, 2, 11, 119, 2, 89, 2, 2, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 2, 185, 2, 284, 2, 2, 37, 315, 2, 226, 20, 272, 2, 40, 29, 152, 60, 181, 2, 30, 50, 553, 362, 80, 119, 12, 21, 846, 2]),\n         list([2, 11, 119, 241, 2, 2, 840, 20, 12, 468, 15, 94, 2, 562, 791, 39, 2, 86, 107, 2, 97, 14, 31, 33, 2, 2, 2, 743, 46, 2, 2, 2, 2, 2, 768, 47, 2, 79, 90, 145, 164, 162, 50, 2, 501, 119, 2, 2, 2, 78, 232, 15, 16, 224, 11, 2, 333, 20, 2, 985, 200, 2, 2, 2, 2, 2, 2, 79, 357, 2, 20, 47, 220, 57, 206, 139, 11, 12, 2, 55, 117, 212, 13, 2, 92, 124, 51, 45, 2, 71, 536, 13, 520, 14, 20, 2, 2, 2, 470]),\n         list([2, 2, 52, 2, 430, 22, 2, 220, 2, 2, 28, 2, 519, 2, 2, 769, 15, 47, 2, 2, 2, 2, 114, 2, 33, 222, 31, 55, 184, 704, 2, 2, 19, 346, 2, 2, 2, 364, 350, 2, 184, 2, 2, 133, 2, 11, 2, 2, 21, 2, 2, 2, 570, 50, 2, 2, 2, 2, 2, 17, 2, 2, 2, 21, 17, 2, 2, 232, 2, 2, 29, 266, 56, 96, 346, 194, 308, 2, 194, 21, 29, 218, 2, 19, 2, 78, 173, 2, 27, 2, 2, 2, 718, 2, 2, 2, 2, 17, 210, 2, 2, 2, 47, 77, 395, 14, 172, 173, 18, 2, 2, 2, 82, 127, 27, 173, 11, 2, 392, 217, 21, 50, 2, 57, 65, 12, 2, 53, 40, 35, 390, 2, 11, 2, 2, 2, 2, 314, 74, 2, 792, 22, 2, 19, 714, 727, 2, 382, 2, 91, 2, 439, 19, 14, 20, 2, 2, 2, 2, 2, 756, 25, 124, 2, 31, 12, 16, 93, 804, 34, 2, 2])],\n        dtype=object),\n  array([0, 1, 1, ..., 0, 0, 0], dtype=int64)))"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ignore the top 10 most frequent words using skip_top\n",
    "\n",
    "imdb.load_data(skip_top=10, num_words=1000, oov_char=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "((array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n         list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n         ...,\n         list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 86527, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 26094, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 74936, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 22459]),\n         list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 21469, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 40691, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 29455, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 21213, 12, 38, 84, 80, 124, 12, 9, 23]),\n         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 29043, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 85010, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 70907, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n        dtype=object),\n  array([1, 0, 0, ..., 0, 1, 0], dtype=int64)),\n (array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 18142, 10698, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 15387, 38, 32, 25, 7944, 451, 202, 14, 6, 717]),\n         list([1, 14, 22, 3443, 6, 176, 7, 5063, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 7216, 10626, 4, 8463, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 16873, 19, 861, 1074, 5, 1987, 17975, 45, 55, 221, 15, 670, 5304, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 5045, 5304, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 8463, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 6936, 185, 132, 1988, 5304, 1799, 488, 2693, 47, 6, 392, 173, 4, 21686, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 5293, 861, 12746, 5, 4182, 30, 3127, 23651, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 5304, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 7836, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 12508, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 5687, 25, 203, 28, 8, 818, 12, 125, 4, 3077]),\n         list([1, 13, 1228, 119, 14, 552, 7, 20, 190, 14, 58, 13, 258, 546, 1786, 8, 1968, 4, 268, 237, 13, 191, 81, 15, 13, 80, 43, 3824, 44, 12, 14, 16, 427, 3192, 4, 183, 15, 593, 19, 4, 351, 362, 26, 55, 646, 21, 4, 1239, 84, 26, 1557, 3755, 13, 244, 6, 2071, 132, 184, 194, 5, 13, 70, 4478, 546, 73, 190, 13, 62, 24, 81, 320, 4, 538, 4, 117, 250, 127, 11, 14, 20, 82, 4, 452, 11, 14, 20, 9, 8654, 19, 41, 476, 8, 4, 213, 7, 9185, 13, 657, 13, 286, 38, 1612, 44, 41, 5, 41, 1729, 88, 13, 62, 28, 900, 510, 4, 509, 51, 6, 612, 59, 16, 193, 61, 4666, 5, 702, 930, 143, 285, 25, 67, 41, 81, 366, 4, 130, 82, 9, 259, 334, 397, 1195, 7, 149, 102, 15, 26, 814, 38, 465, 1627, 31, 70, 983, 67, 51, 9, 112, 814, 17, 35, 311, 75, 26, 11649, 574, 19, 4, 1729, 23, 4, 268, 38, 95, 138, 4, 609, 191, 75, 28, 314, 1772]),\n         ...,\n         list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 16946, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 60664, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518]),\n         list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 28739, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470]),\n         list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 24357, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 18020, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 13226, 21, 4, 7298, 42657, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 25194, 27803, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 20067, 5698, 3406, 718, 21264, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 14274, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 16261, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])],\n        dtype=object),\n  array([0, 1, 0, ..., 0, 0, 0], dtype=int64)))"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limit the sequence lengths to 500 using maxlen\n",
    "\n",
    "imdb.load_data(maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "((array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n         list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n         ...,\n         list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 86527, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 26094, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 74936, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 22459]),\n         list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 21469, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 40691, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 29455, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 21213, 12, 38, 84, 80, 124, 12, 9, 23]),\n         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 29043, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 85010, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 70907, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n        dtype=object),\n  array([1, 0, 0, ..., 0, 1, 0], dtype=int64)),\n (array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 18142, 10698, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 15387, 38, 32, 25, 7944, 451, 202, 14, 6, 717]),\n         list([1, 14, 22, 3443, 6, 176, 7, 5063, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 7216, 10626, 4, 8463, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 16873, 19, 861, 1074, 5, 1987, 17975, 45, 55, 221, 15, 670, 5304, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 5045, 5304, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 8463, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 6936, 185, 132, 1988, 5304, 1799, 488, 2693, 47, 6, 392, 173, 4, 21686, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 5293, 861, 12746, 5, 4182, 30, 3127, 23651, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 5304, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 7836, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 12508, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 5687, 25, 203, 28, 8, 818, 12, 125, 4, 3077]),\n         list([1, 111, 748, 4368, 1133, 33782, 24563, 4, 87, 1551, 1262, 7, 31, 318, 9459, 7, 4, 498, 5076, 748, 63, 29, 5161, 220, 686, 10941, 5, 17, 12, 575, 220, 2507, 17, 6, 185, 132, 24563, 16, 53, 928, 11, 51278, 74, 4, 438, 21, 27, 10044, 589, 8, 22, 107, 20123, 19550, 997, 1638, 8, 35, 2076, 9019, 11, 22, 231, 54, 29, 1706, 29, 100, 18995, 2425, 34, 12998, 8738, 48078, 5, 19353, 98, 31, 2122, 33, 6, 58, 14, 3808, 1638, 8, 4, 365, 7, 2789, 3761, 356, 346, 4, 27608, 1060, 63, 29, 93, 11, 5421, 11, 15236, 33, 6, 58, 54, 1270, 431, 748, 7, 32, 2580, 16, 11, 94, 19469, 10, 10, 4, 993, 45222, 7, 4, 1766, 2634, 2164, 24563, 8, 847, 8, 1450, 121, 31, 7, 27, 86, 2663, 10760, 16, 6, 465, 993, 2006, 30995, 573, 17, 61862, 42, 4, 17345, 37, 473, 6, 711, 6, 8869, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 849, 3711, 53, 33, 2071, 1969, 37, 70, 1144, 4, 5940, 1409, 74, 476, 37, 62, 91, 1329, 169, 4, 1330, 10104, 146, 655, 2212, 5, 258, 12, 184, 10104, 546, 5, 849, 10333, 7, 4, 22, 1436, 18, 631, 1386, 797, 7, 4, 8712, 71, 348, 425, 4320, 1061, 19, 10288, 5, 12141, 11, 661, 8, 339, 17863, 4, 2455, 11434, 7, 4, 1962, 10, 10, 263, 787, 9, 270, 11, 6, 9466, 4, 61862, 48414, 121, 4, 5437, 26, 4434, 19, 68, 1372, 5, 28, 446, 6, 318, 7149, 8, 67, 51, 36, 70, 81, 8, 4392, 2294, 36, 1197, 8, 68411, 25399, 18, 6, 711, 4, 9909, 26, 10296, 1125, 11, 14, 636, 720, 12, 426, 28, 77, 776, 8, 97, 38, 111, 7489, 6175, 168, 1239, 5189, 137, 25399, 18, 27, 173, 9, 2399, 17, 6, 12397, 428, 14657, 232, 11, 4, 8014, 37, 272, 40, 2708, 247, 30, 656, 6, 13182, 54, 25399, 3292, 98, 6, 2840, 40, 558, 37, 6093, 98, 4, 17345, 1197, 15, 14, 9, 57, 4893, 5, 4659, 6, 275, 711, 7937, 25399, 3292, 98, 6, 31036, 10, 10, 6639, 19, 14, 10241, 267, 162, 711, 37, 5900, 752, 98, 4, 17345, 2378, 90, 19, 6, 73284, 7, 36744, 1810, 77553, 4, 4770, 3183, 930, 8, 508, 90, 4, 1317, 8, 4, 48414, 17, 15454, 3965, 1853, 4, 1494, 8, 4468, 189, 4, 31036, 6287, 5774, 4, 4770, 5, 95, 271, 23, 6, 7742, 6063, 21627, 5437, 33, 1526, 6, 425, 3155, 33697, 4535, 1636, 7, 4, 4669, 11966, 469, 4, 4552, 54, 4, 150, 5664, 17345, 280, 53, 68411, 25399, 18, 339, 29, 1978, 27, 7885, 5, 17303, 68, 1830, 19, 6571, 14605, 4, 1515, 7, 263, 65, 2132, 34, 6, 5680, 7489, 43, 159, 29, 9, 4706, 9, 387, 73, 195, 584, 10, 10, 1069, 4, 58, 810, 54, 14, 6078, 117, 22, 16, 93, 5, 1069, 4, 192, 15, 12, 16, 93, 34, 6, 1766, 28228, 33, 4, 5673, 7, 15, 18760, 9252, 3286, 325, 12, 62, 30, 776, 8, 67, 14, 17, 6, 12214, 44, 148, 687, 24563, 203, 42, 203, 24, 28, 69, 32157, 6676, 11, 330, 54, 29, 93, 61862, 21, 845, 14148, 27, 1099, 7, 819, 4, 22, 1407, 17, 6, 14967, 787, 7, 2460, 19569, 61862, 100, 30, 4, 3737, 3617, 3169, 2321, 42, 1898, 11, 4, 3814, 42, 101, 704, 7, 101, 999, 15, 1625, 94, 2926, 180, 5, 9, 9101, 34, 15205, 45, 6, 1429, 22, 60, 6, 1220, 31, 11, 94, 6408, 96, 21, 94, 749, 9, 57, 975]),\n         ...,\n         list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 16946, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 60664, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518]),\n         list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 28739, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470]),\n         list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 24357, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 18020, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 13226, 21, 4, 7298, 42657, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 25194, 27803, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 20067, 5698, 3406, 718, 21264, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 14274, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 16261, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])],\n        dtype=object),\n  array([0, 1, 1, ..., 0, 0, 0], dtype=int64)))"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use '1' as the character that indicates the start of a sequence\n",
    "\n",
    "imdb.load_data(start_char=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the dataset word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 4s 3us/step\n",
      "1654784/1641221 [==============================] - 4s 3us/step\n"
     ]
    }
   ],
   "source": [
    "# Load the imdb word index using get_word_index()\n",
    "\n",
    "imdb_word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'fawn': 34704,\n 'tsukino': 52009,\n 'nunnery': 52010,\n 'sonja': 16819,\n 'vani': 63954,\n 'woods': 1411,\n 'spiders': 16118,\n 'hanging': 2348,\n 'woody': 2292,\n 'trawling': 52011,\n \"hold's\": 52012,\n 'comically': 11310,\n 'localized': 40833,\n 'disobeying': 30571,\n \"'royale\": 52013,\n \"harpo's\": 40834,\n 'canet': 52014,\n 'aileen': 19316,\n 'acurately': 52015,\n \"diplomat's\": 52016,\n 'rickman': 25245,\n 'arranged': 6749,\n 'rumbustious': 52017,\n 'familiarness': 52018,\n \"spider'\": 52019,\n 'hahahah': 68807,\n \"wood'\": 52020,\n 'transvestism': 40836,\n \"hangin'\": 34705,\n 'bringing': 2341,\n 'seamier': 40837,\n 'wooded': 34706,\n 'bravora': 52021,\n 'grueling': 16820,\n 'wooden': 1639,\n 'wednesday': 16821,\n \"'prix\": 52022,\n 'altagracia': 34707,\n 'circuitry': 52023,\n 'crotch': 11588,\n 'busybody': 57769,\n \"tart'n'tangy\": 52024,\n 'burgade': 14132,\n 'thrace': 52026,\n \"tom's\": 11041,\n 'snuggles': 52028,\n 'francesco': 29117,\n 'complainers': 52030,\n 'templarios': 52128,\n '272': 40838,\n '273': 52031,\n 'zaniacs': 52133,\n '275': 34709,\n 'consenting': 27634,\n 'snuggled': 40839,\n 'inanimate': 15495,\n 'uality': 52033,\n 'bronte': 11929,\n 'errors': 4013,\n 'dialogs': 3233,\n \"yomada's\": 52034,\n \"madman's\": 34710,\n 'dialoge': 30588,\n 'usenet': 52036,\n 'videodrome': 40840,\n \"kid'\": 26341,\n 'pawed': 52037,\n \"'girlfriend'\": 30572,\n \"'pleasure\": 52038,\n \"'reloaded'\": 52039,\n \"kazakos'\": 40842,\n 'rocque': 52040,\n 'mailings': 52041,\n 'brainwashed': 11930,\n 'mcanally': 16822,\n \"tom''\": 52042,\n 'kurupt': 25246,\n 'affiliated': 21908,\n 'babaganoosh': 52043,\n \"noe's\": 40843,\n 'quart': 40844,\n 'kids': 362,\n 'uplifting': 5037,\n 'controversy': 7096,\n 'kida': 21909,\n 'kidd': 23382,\n \"error'\": 52044,\n 'neurologist': 52045,\n 'spotty': 18513,\n 'cobblers': 30573,\n 'projection': 9881,\n 'fastforwarding': 40845,\n 'sters': 52046,\n \"eggar's\": 52047,\n 'etherything': 52048,\n 'gateshead': 40846,\n 'airball': 34711,\n 'unsinkable': 25247,\n 'stern': 7183,\n \"cervi's\": 52049,\n 'dnd': 40847,\n 'dna': 11589,\n 'insecurity': 20601,\n \"'reboot'\": 52050,\n 'trelkovsky': 11040,\n 'jaekel': 52051,\n 'sidebars': 52052,\n \"sforza's\": 52053,\n 'distortions': 17636,\n 'mutinies': 52054,\n 'sermons': 30605,\n '7ft': 40849,\n 'boobage': 52055,\n \"o'bannon's\": 52056,\n 'populations': 23383,\n 'chulak': 52057,\n 'mesmerize': 27636,\n 'quinnell': 52058,\n 'yahoo': 10310,\n 'meteorologist': 52060,\n 'beswick': 42580,\n 'boorman': 15496,\n 'voicework': 40850,\n \"ster'\": 52061,\n 'blustering': 22925,\n 'hj': 52062,\n 'intake': 27637,\n 'morally': 5624,\n 'jumbling': 40852,\n 'bowersock': 52063,\n \"'porky's'\": 52064,\n 'gershon': 16824,\n 'ludicrosity': 40853,\n 'coprophilia': 52065,\n 'expressively': 40854,\n \"india's\": 19503,\n \"post's\": 34713,\n 'wana': 52066,\n 'wang': 5286,\n 'wand': 30574,\n 'wane': 25248,\n 'edgeways': 52324,\n 'titanium': 34714,\n 'pinta': 40855,\n 'want': 181,\n 'pinto': 30575,\n 'whoopdedoodles': 52068,\n 'tchaikovsky': 21911,\n 'travel': 2106,\n \"'victory'\": 52069,\n 'copious': 11931,\n 'gouge': 22436,\n \"chapters'\": 52070,\n 'barbra': 6705,\n 'uselessness': 30576,\n \"wan'\": 52071,\n 'assimilated': 27638,\n 'petiot': 16119,\n 'most\\x85and': 52072,\n 'dinosaurs': 3933,\n 'wrong': 355,\n 'seda': 52073,\n 'stollen': 52074,\n 'sentencing': 34715,\n 'ouroboros': 40856,\n 'assimilates': 40857,\n 'colorfully': 40858,\n 'glenne': 27639,\n 'dongen': 52075,\n 'subplots': 4763,\n 'kiloton': 52076,\n 'chandon': 23384,\n \"effect'\": 34716,\n 'snugly': 27640,\n 'kuei': 40859,\n 'welcomed': 9095,\n 'dishonor': 30074,\n 'concurrence': 52078,\n 'stoicism': 23385,\n \"guys'\": 14899,\n \"beroemd'\": 52080,\n 'butcher': 6706,\n \"melfi's\": 40860,\n 'aargh': 30626,\n 'playhouse': 20602,\n 'wickedly': 11311,\n 'fit': 1183,\n 'labratory': 52081,\n 'lifeline': 40862,\n 'screaming': 1930,\n 'fix': 4290,\n 'cineliterate': 52082,\n 'fic': 52083,\n 'fia': 52084,\n 'fig': 34717,\n 'fmvs': 52085,\n 'fie': 52086,\n 'reentered': 52087,\n 'fin': 30577,\n 'doctresses': 52088,\n 'fil': 52089,\n 'zucker': 12609,\n 'ached': 31934,\n 'counsil': 52091,\n 'paterfamilias': 52092,\n 'songwriter': 13888,\n 'shivam': 34718,\n 'hurting': 9657,\n 'effects': 302,\n 'slauther': 52093,\n \"'flame'\": 52094,\n 'sommerset': 52095,\n 'interwhined': 52096,\n 'whacking': 27641,\n 'bartok': 52097,\n 'barton': 8778,\n 'frewer': 21912,\n \"fi'\": 52098,\n 'ingrid': 6195,\n 'stribor': 30578,\n 'approporiately': 52099,\n 'wobblyhand': 52100,\n 'tantalisingly': 52101,\n 'ankylosaurus': 52102,\n 'parasites': 17637,\n 'childen': 52103,\n \"jenkins'\": 52104,\n 'metafiction': 52105,\n 'golem': 17638,\n 'indiscretion': 40863,\n \"reeves'\": 23386,\n \"inamorata's\": 57784,\n 'brittannica': 52107,\n 'adapt': 7919,\n \"russo's\": 30579,\n 'guitarists': 48249,\n 'abbott': 10556,\n 'abbots': 40864,\n 'lanisha': 17652,\n 'magickal': 40866,\n 'mattter': 52108,\n \"'willy\": 52109,\n 'pumpkins': 34719,\n 'stuntpeople': 52110,\n 'estimate': 30580,\n 'ugghhh': 40867,\n 'gameplay': 11312,\n \"wern't\": 52111,\n \"n'sync\": 40868,\n 'sickeningly': 16120,\n 'chiara': 40869,\n 'disturbed': 4014,\n 'portmanteau': 40870,\n 'ineffectively': 52112,\n \"duchonvey's\": 82146,\n \"nasty'\": 37522,\n 'purpose': 1288,\n 'lazers': 52115,\n 'lightened': 28108,\n 'kaliganj': 52116,\n 'popularism': 52117,\n \"damme's\": 18514,\n 'stylistics': 30581,\n 'mindgaming': 52118,\n 'spoilerish': 46452,\n \"'corny'\": 52120,\n 'boerner': 34721,\n 'olds': 6795,\n 'bakelite': 52121,\n 'renovated': 27642,\n 'forrester': 27643,\n \"lumiere's\": 52122,\n 'gaskets': 52027,\n 'needed': 887,\n 'smight': 34722,\n 'master': 1300,\n \"edie's\": 25908,\n 'seeber': 40871,\n 'hiya': 52123,\n 'fuzziness': 52124,\n 'genesis': 14900,\n 'rewards': 12610,\n 'enthrall': 30582,\n \"'about\": 40872,\n \"recollection's\": 52125,\n 'mutilated': 11042,\n 'fatherlands': 52126,\n \"fischer's\": 52127,\n 'positively': 5402,\n '270': 34708,\n 'ahmed': 34723,\n 'zatoichi': 9839,\n 'bannister': 13889,\n 'anniversaries': 52130,\n \"helm's\": 30583,\n \"'work'\": 52131,\n 'exclaimed': 34724,\n \"'unfunny'\": 52132,\n '274': 52032,\n 'feeling': 547,\n \"wanda's\": 52134,\n 'dolan': 33269,\n '278': 52136,\n 'peacoat': 52137,\n 'brawny': 40873,\n 'mishra': 40874,\n 'worlders': 40875,\n 'protags': 52138,\n 'skullcap': 52139,\n 'dastagir': 57599,\n 'affairs': 5625,\n 'wholesome': 7802,\n 'hymen': 52140,\n 'paramedics': 25249,\n 'unpersons': 52141,\n 'heavyarms': 52142,\n 'affaire': 52143,\n 'coulisses': 52144,\n 'hymer': 40876,\n 'kremlin': 52145,\n 'shipments': 30584,\n 'pixilated': 52146,\n \"'00s\": 30585,\n 'diminishing': 18515,\n 'cinematic': 1360,\n 'resonates': 14901,\n 'simplify': 40877,\n \"nature'\": 40878,\n 'temptresses': 40879,\n 'reverence': 16825,\n 'resonated': 19505,\n 'dailey': 34725,\n '2\\x85': 52147,\n 'treize': 27644,\n 'majo': 52148,\n 'kiya': 21913,\n 'woolnough': 52149,\n 'thanatos': 39800,\n 'sandoval': 35734,\n 'dorama': 40882,\n \"o'shaughnessy\": 52150,\n 'tech': 4991,\n 'fugitives': 32021,\n 'teck': 30586,\n \"'e'\": 76128,\n 'doesn’t': 40884,\n 'purged': 52152,\n 'saying': 660,\n \"martians'\": 41098,\n 'norliss': 23421,\n 'dickey': 27645,\n 'dicker': 52155,\n \"'sependipity\": 52156,\n 'padded': 8425,\n 'ordell': 57795,\n \"sturges'\": 40885,\n 'independentcritics': 52157,\n 'tempted': 5748,\n \"atkinson's\": 34727,\n 'hounded': 25250,\n 'apace': 52158,\n 'clicked': 15497,\n \"'humor'\": 30587,\n \"martino's\": 17180,\n \"'supporting\": 52159,\n 'warmongering': 52035,\n \"zemeckis's\": 34728,\n 'lube': 21914,\n 'shocky': 52160,\n 'plate': 7479,\n 'plata': 40886,\n 'sturgess': 40887,\n \"nerds'\": 40888,\n 'plato': 20603,\n 'plath': 34729,\n 'platt': 40889,\n 'mcnab': 52162,\n 'clumsiness': 27646,\n 'altogether': 3902,\n 'massacring': 42587,\n 'bicenntinial': 52163,\n 'skaal': 40890,\n 'droning': 14363,\n 'lds': 8779,\n 'jaguar': 21915,\n \"cale's\": 34730,\n 'nicely': 1780,\n 'mummy': 4591,\n \"lot's\": 18516,\n 'patch': 10089,\n 'kerkhof': 50205,\n \"leader's\": 52164,\n \"'movie\": 27647,\n 'uncomfirmed': 52165,\n 'heirloom': 40891,\n 'wrangle': 47363,\n 'emotion\\x85': 52166,\n \"'stargate'\": 52167,\n 'pinoy': 40892,\n 'conchatta': 40893,\n 'broeke': 41131,\n 'advisedly': 40894,\n \"barker's\": 17639,\n 'descours': 52169,\n 'lots': 775,\n 'lotr': 9262,\n 'irs': 9882,\n 'lott': 52170,\n 'xvi': 40895,\n 'irk': 34731,\n 'irl': 52171,\n 'ira': 6890,\n 'belzer': 21916,\n 'irc': 52172,\n 'ire': 27648,\n 'requisites': 40896,\n 'discipline': 7696,\n 'lyoko': 52964,\n 'extend': 11313,\n 'nature': 876,\n \"'dickie'\": 52173,\n 'optimist': 40897,\n 'lapping': 30589,\n 'superficial': 3903,\n 'vestment': 52174,\n 'extent': 2826,\n 'tendons': 52175,\n \"heller's\": 52176,\n 'quagmires': 52177,\n 'miyako': 52178,\n 'moocow': 20604,\n \"coles'\": 52179,\n 'lookit': 40898,\n 'ravenously': 52180,\n 'levitating': 40899,\n 'perfunctorily': 52181,\n 'lookin': 30590,\n \"lot'\": 40901,\n 'lookie': 52182,\n 'fearlessly': 34873,\n 'libyan': 52184,\n 'fondles': 40902,\n 'gopher': 35717,\n 'wearying': 40904,\n \"nz's\": 52185,\n 'minuses': 27649,\n 'puposelessly': 52186,\n 'shandling': 52187,\n 'decapitates': 31271,\n 'humming': 11932,\n \"'nother\": 40905,\n 'smackdown': 21917,\n 'underdone': 30591,\n 'frf': 40906,\n 'triviality': 52188,\n 'fro': 25251,\n 'bothers': 8780,\n \"'kensington\": 52189,\n 'much': 76,\n 'muco': 34733,\n 'wiseguy': 22618,\n \"richie's\": 27651,\n 'tonino': 40907,\n 'unleavened': 52190,\n 'fry': 11590,\n \"'tv'\": 40908,\n 'toning': 40909,\n 'obese': 14364,\n 'sensationalized': 30592,\n 'spiv': 40910,\n 'spit': 6262,\n 'arkin': 7367,\n 'charleton': 21918,\n 'jeon': 16826,\n 'boardroom': 21919,\n 'doubts': 4992,\n 'spin': 3087,\n 'hepo': 53086,\n 'wildcat': 27652,\n 'venoms': 10587,\n 'misconstrues': 52194,\n 'mesmerising': 18517,\n 'misconstrued': 40911,\n 'rescinds': 52195,\n 'prostrate': 52196,\n 'majid': 40912,\n 'climbed': 16482,\n 'canoeing': 34734,\n 'majin': 52198,\n 'animie': 57807,\n 'sylke': 40913,\n 'conditioned': 14902,\n 'waddell': 40914,\n '3\\x85': 52199,\n 'hyperdrive': 41191,\n 'conditioner': 34735,\n 'bricklayer': 53156,\n 'hong': 2579,\n 'memoriam': 52201,\n 'inventively': 30595,\n \"levant's\": 25252,\n 'portobello': 20641,\n 'remand': 52203,\n 'mummified': 19507,\n 'honk': 27653,\n 'spews': 19508,\n 'visitations': 40915,\n 'mummifies': 52204,\n 'cavanaugh': 25253,\n 'zeon': 23388,\n \"jungle's\": 40916,\n 'viertel': 34736,\n 'frenchmen': 27654,\n 'torpedoes': 52205,\n 'schlessinger': 52206,\n 'torpedoed': 34737,\n 'blister': 69879,\n 'cinefest': 52207,\n 'furlough': 34738,\n 'mainsequence': 52208,\n 'mentors': 40917,\n 'academic': 9097,\n 'stillness': 20605,\n 'academia': 40918,\n 'lonelier': 52209,\n 'nibby': 52210,\n \"losers'\": 52211,\n 'cineastes': 40919,\n 'corporate': 4452,\n 'massaging': 40920,\n 'bellow': 30596,\n 'absurdities': 19509,\n 'expetations': 53244,\n 'nyfiken': 40921,\n 'mehras': 75641,\n 'lasse': 52212,\n 'visability': 52213,\n 'militarily': 33949,\n \"elder'\": 52214,\n 'gainsbourg': 19026,\n 'hah': 20606,\n 'hai': 13423,\n 'haj': 34739,\n 'hak': 25254,\n 'hal': 4314,\n 'ham': 4895,\n 'duffer': 53262,\n 'haa': 52216,\n 'had': 69,\n 'advancement': 11933,\n 'hag': 16828,\n \"hand'\": 25255,\n 'hay': 13424,\n 'mcnamara': 20607,\n \"mozart's\": 52217,\n 'duffel': 30734,\n 'haq': 30597,\n 'har': 13890,\n 'has': 47,\n 'hat': 2404,\n 'hav': 40922,\n 'haw': 30598,\n 'figtings': 52218,\n 'elders': 15498,\n 'underpanted': 52219,\n 'pninson': 52220,\n 'unequivocally': 27655,\n \"barbara's\": 23676,\n \"bello'\": 52222,\n 'indicative': 13000,\n 'yawnfest': 40923,\n 'hexploitation': 52223,\n \"loder's\": 52224,\n 'sleuthing': 27656,\n \"justin's\": 32625,\n \"'ball\": 52225,\n \"'summer\": 52226,\n \"'demons'\": 34938,\n \"mormon's\": 52228,\n \"laughton's\": 34740,\n 'debell': 52229,\n 'shipyard': 39727,\n 'unabashedly': 30600,\n 'disks': 40404,\n 'crowd': 2293,\n 'crowe': 10090,\n \"vancouver's\": 56437,\n 'mosques': 34741,\n 'crown': 6630,\n 'culpas': 52230,\n 'crows': 27657,\n 'surrell': 53347,\n 'flowless': 52232,\n 'sheirk': 52233,\n \"'three\": 40926,\n \"peterson'\": 52234,\n 'ooverall': 52235,\n 'perchance': 40927,\n 'bottom': 1324,\n 'chabert': 53366,\n 'sneha': 52236,\n 'inhuman': 13891,\n 'ichii': 52237,\n 'ursla': 52238,\n 'completly': 30601,\n 'moviedom': 40928,\n 'raddick': 52239,\n 'brundage': 51998,\n 'brigades': 40929,\n 'starring': 1184,\n \"'goal'\": 52240,\n 'caskets': 52241,\n 'willcock': 52242,\n \"threesome's\": 52243,\n \"mosque'\": 52244,\n \"cover's\": 52245,\n 'spaceships': 17640,\n 'anomalous': 40930,\n 'ptsd': 27658,\n 'shirdan': 52246,\n 'obscenity': 21965,\n 'lemmings': 30602,\n 'duccio': 30603,\n \"levene's\": 52247,\n \"'gorby'\": 52248,\n \"teenager's\": 25258,\n 'marshall': 5343,\n 'honeymoon': 9098,\n 'shoots': 3234,\n 'despised': 12261,\n 'okabasho': 52249,\n 'fabric': 8292,\n 'cannavale': 18518,\n 'raped': 3540,\n \"tutt's\": 52250,\n 'grasping': 17641,\n 'despises': 18519,\n \"thief's\": 40931,\n 'rapes': 8929,\n 'raper': 52251,\n \"eyre'\": 27659,\n 'walchek': 52252,\n \"elmo's\": 23389,\n 'perfumes': 40932,\n 'spurting': 21921,\n \"exposition'\\x85\": 52253,\n 'denoting': 52254,\n 'thesaurus': 34743,\n \"shoot'\": 40933,\n 'bonejack': 49762,\n 'simpsonian': 52256,\n 'hebetude': 30604,\n \"hallow's\": 34744,\n 'desperation\\x85': 52257,\n 'incinerator': 34745,\n 'congratulations': 10311,\n 'humbled': 52258,\n \"else's\": 5927,\n 'trelkovski': 40848,\n \"rape'\": 52259,\n \"'chapters'\": 59389,\n '1600s': 52260,\n 'martian': 7256,\n 'nicest': 25259,\n 'eyred': 52262,\n 'passenger': 9460,\n 'disgrace': 6044,\n 'moderne': 52263,\n 'barrymore': 5123,\n 'yankovich': 52264,\n 'moderns': 40934,\n 'studliest': 52265,\n 'bedsheet': 52266,\n 'decapitation': 14903,\n 'slurring': 52267,\n \"'nunsploitation'\": 52268,\n \"'character'\": 34746,\n 'cambodia': 9883,\n 'rebelious': 52269,\n 'pasadena': 27660,\n 'crowne': 40935,\n \"'bedchamber\": 52270,\n 'conjectural': 52271,\n 'appologize': 52272,\n 'halfassing': 52273,\n 'paycheque': 57819,\n 'palms': 20609,\n \"'islands\": 52274,\n 'hawked': 40936,\n 'palme': 21922,\n 'conservatively': 40937,\n 'larp': 64010,\n 'palma': 5561,\n 'smelling': 21923,\n 'aragorn': 13001,\n 'hawker': 52275,\n 'hawkes': 52276,\n 'explosions': 3978,\n 'loren': 8062,\n \"pyle's\": 52277,\n 'shootout': 6707,\n \"mike's\": 18520,\n \"driscoll's\": 52278,\n 'cogsworth': 40938,\n \"britian's\": 52279,\n 'childs': 34747,\n \"portrait's\": 52280,\n 'chain': 3629,\n 'whoever': 2500,\n 'puttered': 52281,\n 'childe': 52282,\n 'maywether': 52283,\n 'chair': 3039,\n \"rance's\": 52284,\n 'machu': 34748,\n 'ballet': 4520,\n 'grapples': 34749,\n 'summerize': 76155,\n 'freelance': 30606,\n \"andrea's\": 52286,\n '\\x91very': 52287,\n 'coolidge': 45882,\n 'mache': 18521,\n 'balled': 52288,\n 'grappled': 40940,\n 'macha': 18522,\n 'underlining': 21924,\n 'macho': 5626,\n 'oversight': 19510,\n 'machi': 25260,\n 'verbally': 11314,\n 'tenacious': 21925,\n 'windshields': 40941,\n 'paychecks': 18560,\n 'jerk': 3399,\n \"good'\": 11934,\n 'prancer': 34751,\n 'prances': 21926,\n 'olympus': 52289,\n 'lark': 21927,\n 'embark': 10788,\n 'gloomy': 7368,\n 'jehaan': 52290,\n 'turaqui': 52291,\n \"child'\": 20610,\n 'locked': 2897,\n 'pranced': 52292,\n 'exact': 2591,\n 'unattuned': 52293,\n 'minute': 786,\n 'skewed': 16121,\n 'hodgins': 40943,\n 'skewer': 34752,\n 'think\\x85': 52294,\n 'rosenstein': 38768,\n 'helmit': 52295,\n 'wrestlemanias': 34753,\n 'hindered': 16829,\n \"martha's\": 30607,\n 'cheree': 52296,\n \"pluckin'\": 52297,\n 'ogles': 40944,\n 'heavyweight': 11935,\n 'aada': 82193,\n 'chopping': 11315,\n 'strongboy': 61537,\n 'hegemonic': 41345,\n 'adorns': 40945,\n 'xxth': 41349,\n 'nobuhiro': 34754,\n 'capitães': 52301,\n 'kavogianni': 52302,\n 'antwerp': 13425,\n 'celebrated': 6541,\n 'roarke': 52303,\n 'baggins': 40946,\n 'cheeseburgers': 31273,\n 'matras': 52304,\n \"nineties'\": 52305,\n \"'craig'\": 52306,\n 'celebrates': 13002,\n 'unintentionally': 3386,\n 'drafted': 14365,\n 'climby': 52307,\n '303': 52308,\n 'oldies': 18523,\n 'climbs': 9099,\n 'honour': 9658,\n 'plucking': 34755,\n '305': 30077,\n 'address': 5517,\n 'menjou': 40947,\n \"'freak'\": 42595,\n 'dwindling': 19511,\n 'benson': 9461,\n 'white’s': 52310,\n 'shamelessness': 40948,\n 'impacted': 21928,\n 'upatz': 52311,\n 'cusack': 3843,\n \"flavia's\": 37570,\n 'effette': 52312,\n 'influx': 34756,\n 'boooooooo': 52313,\n 'dimitrova': 52314,\n 'houseman': 13426,\n 'bigas': 25262,\n 'boylen': 52315,\n 'phillipenes': 52316,\n 'fakery': 40949,\n \"grandpa's\": 27661,\n 'darnell': 27662,\n 'undergone': 19512,\n 'handbags': 52318,\n 'perished': 21929,\n 'pooped': 37781,\n 'vigour': 27663,\n 'opposed': 3630,\n 'etude': 52319,\n \"caine's\": 11802,\n 'doozers': 52320,\n 'photojournals': 34757,\n 'perishes': 52321,\n 'constrains': 34758,\n 'migenes': 40951,\n 'consoled': 30608,\n 'alastair': 16830,\n 'wvs': 52322,\n 'ooooooh': 52323,\n 'approving': 34759,\n 'consoles': 40952,\n 'disparagement': 52067,\n 'futureistic': 52325,\n 'rebounding': 52326,\n \"'date\": 52327,\n 'gregoire': 52328,\n 'rutherford': 21930,\n 'americanised': 34760,\n 'novikov': 82199,\n 'following': 1045,\n 'munroe': 34761,\n \"morita'\": 52329,\n 'christenssen': 52330,\n 'oatmeal': 23109,\n 'fossey': 25263,\n 'livered': 40953,\n 'listens': 13003,\n \"'marci\": 76167,\n \"otis's\": 52333,\n 'thanking': 23390,\n 'maude': 16022,\n 'extensions': 34762,\n 'ameteurish': 52335,\n \"commender's\": 52336,\n 'agricultural': 27664,\n 'convincingly': 4521,\n 'fueled': 17642,\n 'mahattan': 54017,\n \"paris's\": 40955,\n 'vulkan': 52339,\n 'stapes': 52340,\n 'odysessy': 52341,\n 'harmon': 12262,\n 'surfing': 4255,\n 'halloran': 23497,\n 'unbelieveably': 49583,\n \"'offed'\": 52342,\n 'quadrant': 30610,\n 'inhabiting': 19513,\n 'nebbish': 34763,\n 'forebears': 40956,\n 'skirmish': 34764,\n 'ocassionally': 52343,\n \"'resist\": 52344,\n 'impactful': 21931,\n 'spicier': 52345,\n 'touristy': 40957,\n \"'football'\": 52346,\n 'webpage': 40958,\n 'exurbia': 52348,\n 'jucier': 52349,\n 'professors': 14904,\n 'structuring': 34765,\n 'jig': 30611,\n 'overlord': 40959,\n 'disconnect': 25264,\n 'sniffle': 82204,\n 'slimeball': 40960,\n 'jia': 40961,\n 'milked': 16831,\n 'banjoes': 40962,\n 'jim': 1240,\n 'workforces': 52351,\n 'jip': 52352,\n 'rotweiller': 52353,\n 'mundaneness': 34766,\n \"'ninja'\": 52354,\n \"dead'\": 11043,\n \"cipriani's\": 40963,\n 'modestly': 20611,\n \"professor'\": 52355,\n 'shacked': 40964,\n 'bashful': 34767,\n 'sorter': 23391,\n 'overpowering': 16123,\n 'workmanlike': 18524,\n 'henpecked': 27665,\n 'sorted': 18525,\n \"jōb's\": 52357,\n \"'always\": 52358,\n \"'baptists\": 34768,\n 'dreamcatchers': 52359,\n \"'silence'\": 52360,\n 'hickory': 21932,\n 'fun\\x97yet': 52361,\n 'breakumentary': 52362,\n 'didn': 15499,\n 'didi': 52363,\n 'pealing': 52364,\n 'dispite': 40965,\n \"italy's\": 25265,\n 'instability': 21933,\n 'quarter': 6542,\n 'quartet': 12611,\n 'padmé': 52365,\n \"'bleedmedry\": 52366,\n 'pahalniuk': 52367,\n 'honduras': 52368,\n 'bursting': 10789,\n \"pablo's\": 41468,\n 'irremediably': 52370,\n 'presages': 40966,\n 'bowlegged': 57835,\n 'dalip': 65186,\n 'entering': 6263,\n 'newsradio': 76175,\n 'presaged': 54153,\n \"giallo's\": 27666,\n 'bouyant': 40967,\n 'amerterish': 52371,\n 'rajni': 18526,\n 'leeves': 30613,\n 'macauley': 34770,\n 'seriously': 615,\n 'sugercoma': 52372,\n 'grimstead': 52373,\n \"'fairy'\": 52374,\n 'zenda': 30614,\n \"'twins'\": 52375,\n 'realisation': 17643,\n 'highsmith': 27667,\n 'raunchy': 7820,\n 'incentives': 40968,\n 'flatson': 52377,\n 'snooker': 35100,\n 'crazies': 16832,\n 'crazier': 14905,\n 'grandma': 7097,\n 'napunsaktha': 52378,\n 'workmanship': 30615,\n 'reisner': 52379,\n \"sanford's\": 61309,\n '\\x91doña': 52380,\n 'modest': 6111,\n \"everything's\": 19156,\n 'hamer': 40969,\n \"couldn't'\": 52382,\n 'quibble': 13004,\n 'socking': 52383,\n 'tingler': 21934,\n 'gutman': 52384,\n 'lachlan': 40970,\n 'tableaus': 52385,\n 'headbanger': 52386,\n 'spoken': 2850,\n 'cerebrally': 34771,\n \"'road\": 23493,\n 'tableaux': 21935,\n \"proust's\": 40971,\n 'periodical': 40972,\n \"shoveller's\": 52388,\n 'tamara': 25266,\n 'affords': 17644,\n 'concert': 3252,\n \"yara's\": 87958,\n 'someome': 52389,\n 'lingering': 8427,\n \"abraham's\": 41514,\n 'beesley': 34772,\n 'cherbourg': 34773,\n 'kagan': 28627,\n 'snatch': 9100,\n \"miyazaki's\": 9263,\n 'absorbs': 25267,\n \"koltai's\": 40973,\n 'tingled': 64030,\n 'crossroads': 19514,\n 'rehab': 16124,\n 'falworth': 52392,\n 'sequals': 52393,\n ...}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the word index as a dictionary,\n",
    "# accounting for index_from.\n",
    "\n",
    "index_from = 3\n",
    "imdb_word_index = {key: value + index_from for key, value in imdb_word_index.items()}\n",
    "imdb_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "52256"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve a specific word's index\n",
    "\n",
    "imdb_word_index['simpsonian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['this',\n 'film',\n 'was',\n 'just',\n 'brilliant',\n 'casting',\n 'location',\n 'scenery',\n 'story',\n 'direction',\n \"everyone's\",\n 'really',\n 'suited',\n 'the',\n 'part',\n 'they',\n 'played',\n 'and',\n 'you',\n 'could',\n 'just',\n 'imagine',\n 'being',\n 'there',\n 'robert',\n \"redford's\",\n 'is',\n 'an',\n 'amazing',\n 'actor',\n 'and',\n 'now',\n 'the',\n 'same',\n 'being',\n 'director',\n \"norman's\",\n 'father',\n 'came',\n 'from',\n 'the',\n 'same',\n 'scottish',\n 'island',\n 'as',\n 'myself',\n 'so',\n 'i',\n 'loved',\n 'the',\n 'fact',\n 'there',\n 'was',\n 'a',\n 'real',\n 'connection',\n 'with',\n 'this',\n 'film',\n 'the',\n 'witty',\n 'remarks',\n 'throughout',\n 'the',\n 'film',\n 'were',\n 'great',\n 'it',\n 'was',\n 'just',\n 'brilliant',\n 'so',\n 'much',\n 'that',\n 'i',\n 'bought',\n 'the',\n 'film',\n 'as',\n 'soon',\n 'as',\n 'it',\n 'was',\n 'released',\n 'for',\n 'retail',\n 'and',\n 'would',\n 'recommend',\n 'it',\n 'to',\n 'everyone',\n 'to',\n 'watch',\n 'and',\n 'the',\n 'fly',\n 'fishing',\n 'was',\n 'amazing',\n 'really',\n 'cried',\n 'at',\n 'the',\n 'end',\n 'it',\n 'was',\n 'so',\n 'sad',\n 'and',\n 'you',\n 'know',\n 'what',\n 'they',\n 'say',\n 'if',\n 'you',\n 'cry',\n 'at',\n 'a',\n 'film',\n 'it',\n 'must',\n 'have',\n 'been',\n 'good',\n 'and',\n 'this',\n 'definitely',\n 'was',\n 'also',\n 'congratulations',\n 'to',\n 'the',\n 'two',\n 'little',\n \"boy's\",\n 'that',\n 'played',\n 'the',\n \"part's\",\n 'of',\n 'norman',\n 'and',\n 'paul',\n 'they',\n 'were',\n 'just',\n 'brilliant',\n 'children',\n 'are',\n 'often',\n 'left',\n 'out',\n 'of',\n 'the',\n 'praising',\n 'list',\n 'i',\n 'think',\n 'because',\n 'the',\n 'stars',\n 'that',\n 'play',\n 'them',\n 'all',\n 'grown',\n 'up',\n 'are',\n 'such',\n 'a',\n 'big',\n 'profile',\n 'for',\n 'the',\n 'whole',\n 'film',\n 'but',\n 'these',\n 'children',\n 'are',\n 'amazing',\n 'and',\n 'should',\n 'be',\n 'praised',\n 'for',\n 'what',\n 'they',\n 'have',\n 'done',\n \"don't\",\n 'you',\n 'think',\n 'the',\n 'whole',\n 'story',\n 'was',\n 'so',\n 'lovely',\n 'because',\n 'it',\n 'was',\n 'true',\n 'and',\n 'was',\n \"someone's\",\n 'life',\n 'after',\n 'all',\n 'that',\n 'was',\n 'shared',\n 'with',\n 'us',\n 'all']"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View an input sentence\n",
    "\n",
    "inv_imdb_word_index = {value: key for key, value in imdb_word_index.items()}\n",
    "[inv_imdb_word_index[index] for index in x_train[0] if index > index_from]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the sentiment value\n",
    "\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"coding_tutorial_2\"></a>\n",
    "## Padding and Masking Sequence Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the inputs to the maximum length using maxlen\n",
    "\n",
    "padded_x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=300, padding='post', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(25000, 300)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the output data shape\n",
    "\n",
    "padded_x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Masking layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy \n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking expects to see (batch, sequence, features)\n",
    "# Create a dummy feature dimension using expand_dims\n",
    "\n",
    "padded_x_train = np.expand_dims(padded_x_train, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Masking layer \n",
    "\n",
    "tf_x_train = tf.convert_to_tensor(padded_x_train, dtype='float32')\n",
    "making_layer = keras.layers.Masking(mask_value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass tf_x_train to it\n",
    "\n",
    "masked_x_train = making_layer(tf_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(25000, 300, 1), dtype=float32, numpy=\narray([[[1.000e+00],\n        [1.400e+01],\n        [2.200e+01],\n        ...,\n        [0.000e+00],\n        [0.000e+00],\n        [0.000e+00]],\n\n       [[1.000e+00],\n        [1.940e+02],\n        [1.153e+03],\n        ...,\n        [0.000e+00],\n        [0.000e+00],\n        [0.000e+00]],\n\n       [[1.000e+00],\n        [1.400e+01],\n        [4.700e+01],\n        ...,\n        [0.000e+00],\n        [0.000e+00],\n        [0.000e+00]],\n\n       ...,\n\n       [[1.000e+00],\n        [1.100e+01],\n        [6.000e+00],\n        ...,\n        [0.000e+00],\n        [0.000e+00],\n        [0.000e+00]],\n\n       [[1.000e+00],\n        [1.446e+03],\n        [7.079e+03],\n        ...,\n        [0.000e+00],\n        [0.000e+00],\n        [0.000e+00]],\n\n       [[1.000e+00],\n        [1.700e+01],\n        [6.000e+00],\n        ...,\n        [0.000e+00],\n        [0.000e+00],\n        [0.000e+00]]], dtype=float32)>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the dataset\n",
    "\n",
    "masked_x_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(25000, 300), dtype=bool, numpy=\narray([[ True,  True,  True, ..., False, False, False],\n       [ True,  True,  True, ..., False, False, False],\n       [ True,  True,  True, ..., False, False, False],\n       ...,\n       [ True,  True,  True, ..., False, False, False],\n       [ True,  True,  True, ..., False, False, False],\n       [ True,  True,  True, ..., False, False, False]])>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the ._keras_mask for the dataset\n",
    "\n",
    "masked_x_train._keras_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_3\"></a>\n",
    "## The Embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and apply an `Embedding` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding layer using layers.Embedding\n",
    "# Specify input_dim, output_dim, input_length\n",
    "\n",
    "embadding_layer = keras.layers.Embedding(input_dim=501, output_dim=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 4, 1, 16), dtype=float32, numpy=\narray([[[[-0.04273875, -0.04465742,  0.0287236 , -0.01091051,\n          -0.02989529, -0.03992537, -0.04954729, -0.02891388,\n           0.00698565,  0.02145154,  0.01922313,  0.04291142,\n           0.0137721 ,  0.04771248, -0.00611737,  0.04394361]],\n\n        [[ 0.04779637,  0.02389714, -0.04893187,  0.0375867 ,\n           0.01612054,  0.00370157,  0.02974105,  0.03388169,\n           0.03642387, -0.02935883, -0.04077767,  0.00021851,\n           0.02233993,  0.03390006,  0.02620851, -0.00717572]],\n\n        [[ 0.01991505,  0.01045588,  0.04039833, -0.03334473,\n          -0.04028636, -0.04769758, -0.01980908,  0.0396787 ,\n          -0.03213881,  0.0486979 ,  0.03941811, -0.01203724,\n          -0.0362131 ,  0.04756066, -0.02832707,  0.00866859]],\n\n        [[ 0.01922802, -0.00556922, -0.02115997, -0.0194272 ,\n           0.0378556 , -0.04577959,  0.02035215,  0.0228614 ,\n           0.04556142,  0.00266675, -0.04632602,  0.03150418,\n          -0.01173477,  0.04643321, -0.02968388, -0.04628581]]]],\n      dtype=float32)>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect an Embedding layer output for a fixed input\n",
    "# Expects an input of shape (batch, sequence, feature)\n",
    "\n",
    "sequance_of_indices = tf.constant([[[0], [1], [5], [500]]])\n",
    "sequance_of_embadding = embadding_layer(sequance_of_indices)\n",
    "sequance_of_embadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.04273875, -0.04465742,  0.0287236 , ...,  0.04771248,\n        -0.00611737,  0.04394361],\n       [ 0.04779637,  0.02389714, -0.04893187, ...,  0.03390006,\n         0.02620851, -0.00717572],\n       [ 0.00293254,  0.00769534, -0.00696599, ..., -0.03376386,\n        -0.04846593, -0.03140409],\n       ...,\n       [-0.04959455, -0.01156616, -0.02569532, ..., -0.03893064,\n         0.00935585,  0.01845231],\n       [-0.04949873,  0.01419321, -0.00417424, ...,  0.04305756,\n         0.0024427 ,  0.015445  ],\n       [ 0.01922802, -0.00556922, -0.02115997, ...,  0.04643321,\n        -0.02968388, -0.04628581]], dtype=float32)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the Embedding layer weights using get_weights()\n",
    "\n",
    "embadding_layer.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([-0.00559667, -0.02749426, -0.0385205 , -0.0029053 , -0.03085314,\n        0.00955486,  0.03628239, -0.04974259, -0.03494241,  0.0144037 ,\n       -0.01297567, -0.00670594,  0.01994615,  0.03348816, -0.03531346,\n        0.04622756], dtype=float32)"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the embedding for the 14th index\n",
    "\n",
    "embadding_layer.get_weights()[0][14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and apply an `Embedding` layer that uses `mask_zero=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a layer that uses the mask_zero kwarg\n",
    "\n",
    "masking_embedding_layer = keras.layers.Embedding(input_dim=501, output_dim=16, mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 4, 1), dtype=bool, numpy=\narray([[[False],\n        [ True],\n        [ True],\n        [ True]]])>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply this layer to the sequence and see the _keras_mask property\n",
    "\n",
    "masked_sequence_of_embaddings = masking_embedding_layer(sequance_of_indices)\n",
    "masked_sequence_of_embaddings._keras_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"coding_tutorial_4\"></a>\n",
    "## The Embedding Projector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and preprocess the IMDb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to load and preprocess the IMDB dataset\n",
    "\n",
    "def get_and_pad_imdb_dataset(num_words=10000, maxlen=None, index_from=2):\n",
    "    from tensorflow.keras.datasets import imdb\n",
    "\n",
    "    # Load the reviews\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(path='imdb.npz',\n",
    "                                                          num_words=num_words,\n",
    "                                                          skip_top=0,\n",
    "                                                          maxlen=maxlen,\n",
    "                                                          start_char=1,\n",
    "                                                          oov_char=2,\n",
    "                                                          index_from=index_from)\n",
    "\n",
    "    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        maxlen=None,\n",
    "                                                        padding='pre',\n",
    "                                                        truncating='pre',\n",
    "                                                        value=0)\n",
    "    \n",
    "    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                           maxlen=None,\n",
    "                                                           padding='pre',\n",
    "                                                           truncating='pre',\n",
    "                                                           value=0)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "(x_train,y_train), (x_test, y_test) = get_and_pad_imdb_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to get the dataset word index\n",
    "\n",
    "def get_imdb_word_index(num_words=10000, index_from=2):\n",
    "    imdb_word_index = tf.keras.datasets.imdb.get_word_index(\n",
    "                                        path='imdb_word_index.json')\n",
    "    imdb_word_index = {key: value + index_from for\n",
    "                       key, value in imdb_word_index.items() if value <= num_words-index_from}\n",
    "    return imdb_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word index\n",
    "\n",
    "imdb_word_index = get_imdb_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swap the keys and values of the word index\n",
    "\n",
    "inv_imdb_word_index = {value:key for key, value in imdb_word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['i',\n 'am',\n 'a',\n 'great',\n 'fan',\n 'of',\n 'david',\n 'lynch',\n 'and',\n 'have',\n 'everything',\n 'that',\n \"he's\",\n 'made',\n 'on',\n 'dvd',\n 'except',\n 'for',\n 'hotel',\n 'room',\n 'the',\n '2',\n 'hour',\n 'twin',\n 'peaks',\n 'movie',\n 'so',\n 'when',\n 'i',\n 'found',\n 'out',\n 'about',\n 'this',\n 'i',\n 'immediately',\n 'grabbed',\n 'it',\n 'and',\n 'and',\n 'what',\n 'is',\n 'this',\n \"it's\",\n 'a',\n 'bunch',\n 'of',\n 'drawn',\n 'black',\n 'and',\n 'white',\n 'cartoons',\n 'that',\n 'are',\n 'loud',\n 'and',\n 'foul',\n 'mouthed',\n 'and',\n 'unfunny',\n 'maybe',\n 'i',\n \"don't\",\n 'know',\n \"what's\",\n 'good',\n 'but',\n 'maybe',\n 'this',\n 'is',\n 'just',\n 'a',\n 'bunch',\n 'of',\n 'crap',\n 'that',\n 'was',\n 'on',\n 'the',\n 'public',\n 'under',\n 'the',\n 'name',\n 'of',\n 'david',\n 'lynch',\n 'to',\n 'make',\n 'a',\n 'few',\n 'bucks',\n 'too',\n 'let',\n 'me',\n 'make',\n 'it',\n 'clear',\n 'that',\n 'i',\n \"didn't\",\n 'care',\n 'about',\n 'the',\n 'foul',\n 'language',\n 'part',\n 'but',\n 'had',\n 'to',\n 'keep',\n 'the',\n 'sound',\n 'because',\n 'my',\n 'neighbors',\n 'might',\n 'have',\n 'all',\n 'in',\n 'all',\n 'this',\n 'is',\n 'a',\n 'highly',\n 'disappointing',\n 'release',\n 'and',\n 'may',\n 'well',\n 'have',\n 'just',\n 'been',\n 'left',\n 'in',\n 'the',\n 'box',\n 'set',\n 'as',\n 'a',\n 'curiosity',\n 'i',\n 'highly',\n 'recommend',\n 'you',\n \"don't\",\n 'spend',\n 'your',\n 'money',\n 'on',\n 'this',\n '2',\n 'out',\n 'of',\n '10']"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first dataset example sentence\n",
    "\n",
    "[inv_imdb_word_index[index] for index in x_train[100] if index > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build an Embedding layer into a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum token value\n",
    "\n",
    "max_index_value = max(imdb_word_index.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify an embedding dimension\n",
    "\n",
    "embadding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model using Sequential:\n",
    "#     1. Embedding layer\n",
    "#     2. GlobalAveragePooling1D\n",
    "#     3. Dense\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=max_index_value + 1, output_dim=embadding_dim, mask_zero=False),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional API refresher: use the Model to build the same model\n",
    "\n",
    "review_sequence = keras.Input((None, ))\n",
    "embedding_sequence = keras.layers.Embedding(input_dim=max_index_value+1, output_dim=embadding_dim)(review_sequence)\n",
    "average_embadding = keras.layers.GlobalAveragePooling1D()(embedding_sequence)\n",
    "positive_probability = keras.layers.Dense(units=1, activation='sigmoid')(average_embadding)\n",
    "\n",
    "model = keras.models.Model(review_sequence, positive_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, None, 16)          160016    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 16)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile, train, and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with a binary cross-entropy loss\n",
    "\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 9s 9ms/step - loss: 0.6898 - accuracy: 0.5570 - val_loss: 0.6836 - val_accuracy: 0.6219\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.6693 - accuracy: 0.6780 - val_loss: 0.6485 - val_accuracy: 0.7500\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.6250 - accuracy: 0.7605 - val_loss: 0.5977 - val_accuracy: 0.7484\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.5730 - accuracy: 0.7933 - val_loss: 0.5490 - val_accuracy: 0.7875\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.5242 - accuracy: 0.8213 - val_loss: 0.5085 - val_accuracy: 0.7937\n"
     ]
    }
   ],
   "source": [
    "# Train the model using .fit(), savng its history\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test), validation_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1008x360 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training and validation accuracy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "acc      = history_dict['accuracy']\n",
    "val_acc  = history_dict['val_accuracy']\n",
    "loss     = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(epochs, acc, marker='.', label='Training acc')\n",
    "plt.plot(epochs, val_acc, marker='.', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The TensorFlow embedding projector\n",
    "\n",
    "The Tensorflow embedding projector can be found [here](https://projector.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the embedding layer's weights from the trained model\n",
    "\n",
    "weights = model.layers[1].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the word Embeddings to tsv files\n",
    "# Two files: \n",
    "#     one contains the embedding labels (meta.tsv),\n",
    "#     one contains the embeddings (vecs.tsv)\n",
    "\n",
    "import io\n",
    "from os import path\n",
    "\n",
    "out_v = io.open(path.join('../../data', 'vecs.tsv'), 'w', encoding='utf-8')\n",
    "out_m = io.open(path.join('../../data', 'meta.tsv'), 'w', encoding='utf-8')\n",
    "\n",
    "k = 0\n",
    "\n",
    "for word, token in imdb_word_index.items():\n",
    "    if k != 0:\n",
    "        out_m.write('\\n')\n",
    "        out_v.write('\\n')\n",
    "    \n",
    "    out_v.write('\\t'.join([str(x) for x in weights[token]]))\n",
    "    out_m.write(word)\n",
    "    k += 1\n",
    "    \n",
    "out_v.close()\n",
    "out_m.close()\n",
    "# beware large collections of embeddings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"coding_tutorial_5\"></a>\n",
    "## Recurrent neural network layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize and pass an input to a SimpleRNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SimpleRNN layer and test it\n",
    "\n",
    "simplernn_layers = keras.layers.SimpleRNN(units=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 16), dtype=float32, numpy=\narray([[-1.       ,  1.       , -0.9904304, -1.       ,  1.       ,\n        -1.       , -0.999399 ,  1.       , -0.9821164,  0.9993913,\n         1.       ,  1.       ,  1.       , -0.9999976,  1.       ,\n        -1.       ]], dtype=float32)>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that only the final cell output is returned\n",
    "\n",
    "sequence = tf.constant([[[1., 1.], [2.,2.], [56., -100.]]])\n",
    "layer_output = simplernn_layers(sequence)\n",
    "layer_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and transform the IMDB review sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to load and preprocess the IMDB dataset\n",
    "\n",
    "def get_and_pad_imdb_dataset(num_words=10000, maxlen=None, index_from=2):\n",
    "    from tensorflow.keras.datasets import imdb\n",
    "\n",
    "    # Load the reviews\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(path='imdb.npz',\n",
    "                                                          num_words=num_words,\n",
    "                                                          skip_top=0,\n",
    "                                                          maxlen=maxlen,\n",
    "                                                          start_char=1,\n",
    "                                                          oov_char=2,\n",
    "                                                          index_from=index_from)\n",
    "\n",
    "    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        maxlen=None,\n",
    "                                                        padding='pre',\n",
    "                                                        truncating='pre',\n",
    "                                                        value=0)\n",
    "    \n",
    "    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                           maxlen=None,\n",
    "                                                           padding='pre',\n",
    "                                                           truncating='pre',\n",
    "                                                           value=0)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = get_and_pad_imdb_dataset(maxlen=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to get the dataset word index\n",
    "\n",
    "def get_imdb_word_index(num_words=10000, index_from=2):\n",
    "    imdb_word_index = tf.keras.datasets.imdb.get_word_index(\n",
    "                                        path='imdb_word_index.json')\n",
    "    imdb_word_index = {key: value + index_from for\n",
    "                       key, value in imdb_word_index.items() if value <= num_words-index_from}\n",
    "    return imdb_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word index using get_imdb_word_index()\n",
    "\n",
    "imdb_word_index = get_imdb_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a recurrent neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum index value\n",
    "\n",
    "max_index_value = max(imdb_word_index.values())\n",
    "embadding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Sequential, build the model:\n",
    "# 1. Embedding.\n",
    "# 2. LSTM.\n",
    "# 3. Dense.\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=max_index_value + 1, output_dim=embadding_dim, mask_zero=True),\n",
    "    keras.layers.LSTM(16),\n",
    "    keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with binary cross-entropy loss\n",
    "\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "536/536 [==============================] - 799s 1s/step - loss: 0.4187 - accuracy: 0.8050\n",
      "Epoch 2/3\n",
      "536/536 [==============================] - 735s 1s/step - loss: 0.2228 - accuracy: 0.9176\n",
      "Epoch 3/3\n",
      "152/536 [=======>......................] - ETA: 9:07 - loss: 0.1410 - accuracy: 0.9550"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_840/1076854799.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Fit the model and save its training history\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mhistory\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m32\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mc:\\program files\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     63\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 64\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     65\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint: disable=broad-except\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     66\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\program files\\python39\\lib\\site-packages\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1214\u001B[0m                 _r=1):\n\u001B[0;32m   1215\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1216\u001B[1;33m               \u001B[0mtmp_logs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1217\u001B[0m               \u001B[1;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1218\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\program files\\python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    149\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 150\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    151\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\program files\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    908\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    909\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jit_compile\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 910\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    911\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    912\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\program files\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m_call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    940\u001B[0m       \u001B[1;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    941\u001B[0m       \u001B[1;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 942\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=not-callable\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    943\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    944\u001B[0m       \u001B[1;31m# Release the lock early so that multiple threads can perform the call\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\program files\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   3128\u001B[0m       (graph_function,\n\u001B[0;32m   3129\u001B[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001B[1;32m-> 3130\u001B[1;33m     return graph_function._call_flat(\n\u001B[0m\u001B[0;32m   3131\u001B[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001B[0;32m   3132\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\program files\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1957\u001B[0m         and executing_eagerly):\n\u001B[0;32m   1958\u001B[0m       \u001B[1;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1959\u001B[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001B[0m\u001B[0;32m   1960\u001B[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0;32m   1961\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001B[1;32mc:\\program files\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    596\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0m_InterpolateFunctionError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    597\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mcancellation_manager\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 598\u001B[1;33m           outputs = execute.execute(\n\u001B[0m\u001B[0;32m    599\u001B[0m               \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msignature\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    600\u001B[0m               \u001B[0mnum_outputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_num_outputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\program files\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     56\u001B[0m   \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     57\u001B[0m     \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 58\u001B[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0m\u001B[0;32m     59\u001B[0m                                         inputs, attrs, num_outputs)\n\u001B[0;32m     60\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Fit the model and save its training history\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "acc      = history_dict['accuracy']\n",
    "val_acc  = history_dict['val_accuracy']\n",
    "loss     = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(epochs, acc, marker='.', label='Training acc')\n",
    "plt.plot(epochs, val_acc, marker='.', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first test data example sentence\n",
    "# (invert the word index)\n",
    "\n",
    "inv_imdb_word_index = {value:key for key, value in imdb_word_index.items()}\n",
    "[inv_imdb_word_index[index] for index in x_train[100] if index > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model prediction using model.predict()\n",
    "\n",
    "model.predict(x_test[None,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the corresponding label\n",
    "\n",
    "y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"coding_tutorial_6\"></a>\n",
    "## Stacked RNNs and the Bidirectional wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and transform the IMDB review sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to load and preprocess the IMDB dataset\n",
    "\n",
    "def get_and_pad_imdb_dataset(num_words=10000, maxlen=None, index_from=2):\n",
    "    from tensorflow.keras.datasets import imdb\n",
    "\n",
    "    # Load the reviews\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(path='imdb.npz',\n",
    "                                                          num_words=num_words,\n",
    "                                                          skip_top=0,\n",
    "                                                          maxlen=maxlen,\n",
    "                                                          start_char=1,\n",
    "                                                          oov_char=2,\n",
    "                                                          index_from=index_from)\n",
    "\n",
    "    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        maxlen=None,\n",
    "                                                        padding='pre',\n",
    "                                                        truncating='pre',\n",
    "                                                        value=0)\n",
    "    \n",
    "    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                           maxlen=None,\n",
    "                                                           padding='pre',\n",
    "                                                           truncating='pre',\n",
    "                                                           value=0)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to get the dataset word index\n",
    "\n",
    "def get_imdb_word_index(num_words=10000, index_from=2):\n",
    "    imdb_word_index = tf.keras.datasets.imdb.get_word_index(\n",
    "                                        path='imdb_word_index.json')\n",
    "    imdb_word_index = {key: value + index_from for\n",
    "                       key, value in imdb_word_index.items() if value <= num_words-index_from}\n",
    "    return imdb_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word index using get_imdb_word_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build stacked and bidirectional recurrent models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum index value and specify an embedding dimension\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Sequential, build a stacked LSTM model via return_sequences=True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Sequential, build a bidirectional RNN with merge_mode='sum'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model featuring both stacked recurrent layers and a bidirectional layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, saving its history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "acc      = history_dict['accuracy']\n",
    "val_acc  = history_dict['val_accuracy']\n",
    "loss     = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(epochs, acc, marker='.', label='Training acc')\n",
    "plt.plot(epochs, val_acc, marker='.', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0, 1);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}